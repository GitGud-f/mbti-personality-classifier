{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas numpy scikit-learn imbalanced-learn gensim contractions emoji nltk tqdm joblib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import contractions\n",
        "import emoji\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from gensim.models import KeyedVectors\n",
        "import gdown\n",
        "import logging\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "WNL = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing functions\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean raw text by removing URLs, emojis, mentions, and MBTI codes.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?\\S+|www\\S+', '', text)\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r'@\\w+|#', '', text)\n",
        "    text = re.sub(r\"[^a-z\\']\", ' ', text)\n",
        "    # Remove MBTI type codes\n",
        "    text = re.sub(r'\\b(I|E)(N|S)(F|T)(J|P)(S)?\\b', '', text, flags=re.IGNORECASE)\n",
        "    # Remove common footer\n",
        "    text = re.sub(r'\\bsent (from )?my \\w+(\\s\\w+)? using tapatalk\\b', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'w w w', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def preprocess_posts(posts_str: str) -> str:\n",
        "    \"\"\"Split multi-post string (separated by '|||'), clean each, and join.\"\"\"\n",
        "    posts = posts_str.split('|||')\n",
        "    cleaned_posts = [clean_text(post) for post in posts]\n",
        "    joined = ' '.join(cleaned_posts)\n",
        "    return re.sub(r'\\s+', ' ', joined).strip()\n",
        "\n",
        "def tokens_with_lemma(text: str):\n",
        "    \"\"\"Tokenize with lemmatization.\"\"\"\n",
        "    fixed = contractions.fix(text)\n",
        "    tokens = word_tokenize(fixed)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
        "    lemmatized = [WNL.lemmatize(word.lower()) for word in filtered_tokens]\n",
        "    return lemmatized\n",
        "\n",
        "def encode_mbti_type(mbti: str):\n",
        "    \"\"\"Encode MBTI type as 4 binary dimensions.\"\"\"\n",
        "    return (\n",
        "        1 if mbti[0] == 'I' else 0,\n",
        "        1 if mbti[1] == 'N' else 0,\n",
        "        1 if mbti[2] == 'F' else 0,\n",
        "        1 if mbti[3] == 'J' else 0,\n",
        "    )\n",
        "\n",
        "# Download pretrained Word2Vec if not present\n",
        "def download_pretrained_w2v():\n",
        "    file_id = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'  # Google News vectors\n",
        "    destination = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "    if not os.path.exists(destination):\n",
        "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", destination, quiet=False)\n",
        "    if not os.path.exists('GoogleNews-vectors-negative300.bin'):\n",
        "        !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "    return 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "# Function to average word embeddings\n",
        "def average_embedding(tokens, model, embedding_size=300):\n",
        "    valid_embeddings = []\n",
        "    for token in tokens:\n",
        "        if token in model:\n",
        "            valid_embeddings.append(model[token])\n",
        "    if not valid_embeddings:\n",
        "        return np.zeros(embedding_size)\n",
        "    return np.mean(valid_embeddings, axis=0)\n",
        "\n",
        "# Prepare embeddings\n",
        "def prepare_embeddings(df, model, embedding_size=300):\n",
        "    embeddings = []\n",
        "    for tokens in tqdm(df['tokens']):\n",
        "        emb = average_embedding(tokens, model, embedding_size)\n",
        "        embeddings.append(emb)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load raw data\n",
        "    logger.info(\"ðŸ“‚ Loading raw data...\")\n",
        "    df = pd.read_csv('mbti_1.csv')  # Adjust path if needed, e.g., '/content/mbti_1.csv' in Colab\n",
        "\n",
        "    # Preprocess\n",
        "    logger.info(\"ðŸ§¹ Preprocessing posts...\")\n",
        "    df['cleaned_posts'] = df['posts'].apply(preprocess_posts)\n",
        "    df['tokens'] = df['cleaned_posts'].apply(tokens_with_lemma)\n",
        "\n",
        "    # Encode MBTI\n",
        "    logger.info(\"ðŸ”¢ Encoding MBTI types...\")\n",
        "    df[['IE', 'NS', 'FT', 'JP']] = df.apply(\n",
        "        lambda row: pd.Series(encode_mbti_type(row['type'])), axis=1\n",
        "    )\n",
        "\n",
        "    # Split data\n",
        "    train_df, test_df = train_test_split(\n",
        "        df, test_size=0.2, random_state=42, stratify=df['type']\n",
        "    )\n",
        "\n",
        "    # Download and load pretrained Word2Vec\n",
        "    w2v_path = download_pretrained_w2v()\n",
        "    logger.info(\"ðŸ§  Loading pretrained Word2Vec...\")\n",
        "    model = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
        "\n",
        "    # Generate embeddings\n",
        "    logger.info(\"ðŸ“ Generating embeddings for train...\")\n",
        "    X_train_emb = prepare_embeddings(train_df, model)\n",
        "\n",
        "    logger.info(\"ðŸ“ Generating embeddings for test...\")\n",
        "    X_test_emb = prepare_embeddings(test_df, model)\n",
        "\n",
        "    # Results storage\n",
        "    results_binary = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}\n",
        "    results_binary_balanced = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}\n",
        "\n",
        "    # Train and evaluate\n",
        "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
        "        logger.info(f\"ðŸ§  Training binary model for {dim} with SMOTE...\")\n",
        "        y_train_bin = train_df[dim]\n",
        "        y_test_bin = test_df[dim]\n",
        "\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_res, y_train_res = smote.fit_resample(X_train_emb, y_train_bin)\n",
        "\n",
        "        clf = LogisticRegression(max_iter=1000)\n",
        "        clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "        y_pred_bin = clf.predict(X_test_emb)\n",
        "        acc_bin = accuracy_score(y_test_bin, y_pred_bin)\n",
        "        f1_bin = f1_score(y_test_bin, y_pred_bin, average='weighted')\n",
        "        results_binary[dim] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
        "\n",
        "        print(f\"{dim} - Accuracy: {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
        "        print(classification_report(y_test_bin, y_pred_bin))\n",
        "\n",
        "        # Save model (optional in Colab)\n",
        "        clf_path = f'binary_{dim}_pretrained_w2v.pkl'\n",
        "        joblib.dump(clf, clf_path)\n",
        "\n",
        "    # Balanced test evaluation\n",
        "    logger.info(\"\\nðŸ” Evaluating on SMOTE-balanced test...\")\n",
        "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
        "        y_test_bin = test_df[dim]\n",
        "        clf = joblib.load(f'binary_{dim}_pretrained_w2v.pkl')\n",
        "\n",
        "        smote_test = SMOTE(random_state=42)\n",
        "        X_test_res, y_test_res = smote_test.fit_resample(X_test_emb, y_test_bin)\n",
        "\n",
        "        y_pred_bin = clf.predict(X_test_res)\n",
        "\n",
        "        acc_bin = accuracy_score(y_test_res, y_pred_bin)\n",
        "        f1_bin = f1_score(y_test_res, y_pred_bin, average='weighted')\n",
        "\n",
        "        results_binary_balanced[dim] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
        "        print(f\"{dim} - Accuracy (on SMOTE-balanced test): {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
        "        print(classification_report(y_test_res, y_pred_bin))\n",
        "\n",
        "    # Compare results\n",
        "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
        "        print(f\"\\nðŸ“Š Binary {dim} Results:\")\n",
        "        print(f\"Original Test: Accuracy={results_binary[dim]['accuracy']:.4f}, F1={results_binary[dim]['f1']:.4f}\")\n",
        "        print(f\"Balanced Test: Accuracy={results_binary_balanced[dim]['accuracy']:.4f}, F1={results_binary_balanced[dim]['f1']:.4f}\")\n",
        "\n",
        "    logger.info(\"âœ… Process complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pPmuvmCOdG90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf45be5-b108-4279-b31c-60e8f7932771"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.12/dist-packages (0.1.73)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.12/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.12/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
            "From (redirected): https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&confirm=t&uuid=bcb8d786-8faa-4e17-af25-6492621649b1\n",
            "To: /content/GoogleNews-vectors-negative300.bin.gz\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65G/1.65G [00:21<00:00, 76.4MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6940/6940 [00:11<00:00, 601.63it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1735/1735 [00:03<00:00, 513.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IE - Accuracy: 0.6824, F1: 0.7046\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.65      0.49       401\n",
            "           1       0.87      0.69      0.77      1334\n",
            "\n",
            "    accuracy                           0.68      1735\n",
            "   macro avg       0.63      0.67      0.63      1735\n",
            "weighted avg       0.76      0.68      0.70      1735\n",
            "\n",
            "NS - Accuracy: 0.6692, F1: 0.7191\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.66      0.36       240\n",
            "           1       0.92      0.67      0.78      1495\n",
            "\n",
            "    accuracy                           0.67      1735\n",
            "   macro avg       0.58      0.66      0.57      1735\n",
            "weighted avg       0.83      0.67      0.72      1735\n",
            "\n",
            "FT - Accuracy: 0.7418, F1: 0.7422\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.75      0.73       796\n",
            "           1       0.77      0.74      0.76       939\n",
            "\n",
            "    accuracy                           0.74      1735\n",
            "   macro avg       0.74      0.74      0.74      1735\n",
            "weighted avg       0.74      0.74      0.74      1735\n",
            "\n",
            "JP - Accuracy: 0.6300, F1: 0.6336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.64      0.67      1048\n",
            "           1       0.53      0.62      0.57       687\n",
            "\n",
            "    accuracy                           0.63      1735\n",
            "   macro avg       0.62      0.63      0.62      1735\n",
            "weighted avg       0.64      0.63      0.63      1735\n",
            "\n",
            "IE - Accuracy (on SMOTE-balanced test): 0.6934, F1: 0.6934\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.70      0.69      1334\n",
            "           1       0.69      0.69      0.69      1334\n",
            "\n",
            "    accuracy                           0.69      2668\n",
            "   macro avg       0.69      0.69      0.69      2668\n",
            "weighted avg       0.69      0.69      0.69      2668\n",
            "\n",
            "NS - Accuracy (on SMOTE-balanced test): 0.6666, F1: 0.6665\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.66      0.67      1495\n",
            "           1       0.67      0.67      0.67      1495\n",
            "\n",
            "    accuracy                           0.67      2990\n",
            "   macro avg       0.67      0.67      0.67      2990\n",
            "weighted avg       0.67      0.67      0.67      2990\n",
            "\n",
            "FT - Accuracy (on SMOTE-balanced test): 0.7396, F1: 0.7396\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.74      0.74       939\n",
            "           1       0.74      0.74      0.74       939\n",
            "\n",
            "    accuracy                           0.74      1878\n",
            "   macro avg       0.74      0.74      0.74      1878\n",
            "weighted avg       0.74      0.74      0.74      1878\n",
            "\n",
            "JP - Accuracy (on SMOTE-balanced test): 0.6336, F1: 0.6336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.64      0.63      1048\n",
            "           1       0.63      0.63      0.63      1048\n",
            "\n",
            "    accuracy                           0.63      2096\n",
            "   macro avg       0.63      0.63      0.63      2096\n",
            "weighted avg       0.63      0.63      0.63      2096\n",
            "\n",
            "\n",
            "ðŸ“Š Binary IE Results:\n",
            "Original Test: Accuracy=0.6824, F1=0.7046\n",
            "Balanced Test: Accuracy=0.6934, F1=0.6934\n",
            "\n",
            "ðŸ“Š Binary NS Results:\n",
            "Original Test: Accuracy=0.6692, F1=0.7191\n",
            "Balanced Test: Accuracy=0.6666, F1=0.6665\n",
            "\n",
            "ðŸ“Š Binary FT Results:\n",
            "Original Test: Accuracy=0.7418, F1=0.7422\n",
            "Balanced Test: Accuracy=0.7396, F1=0.7396\n",
            "\n",
            "ðŸ“Š Binary JP Results:\n",
            "Original Test: Accuracy=0.6300, F1=0.6336\n",
            "Balanced Test: Accuracy=0.6336, F1=0.6336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wrgbajZyoBDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}