{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to average word embeddings for a list of tokens\n",
    "def average_embedding(tokens, model, embedding_size=100):\n",
    "    \"\"\"\n",
    "    Compute average embedding for a list of tokens.\n",
    "    Ignores words not in vocabulary.\n",
    "    \"\"\"\n",
    "    valid_embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            valid_embeddings.append(model.wv[token])\n",
    "    if not valid_embeddings:\n",
    "        return np.zeros(embedding_size)\n",
    "    return np.mean(valid_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to prepare embeddings for a dataframe column\n",
    "def prepare_embeddings(df, variant, model, embedding_size=100):\n",
    "    \"\"\"\n",
    "    Generate averaged embeddings for each row's tokens.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for tokens in tqdm(df[f'tokens_{variant}']):\n",
    "        emb = average_embedding(tokens, model, embedding_size)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b27cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training and evaluation function\n",
    "def train_and_evaluate(embedding_type='word2vec', variant='with_lemma', embedding_size=100, epochs=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using specified embeddings.\n",
    "    Supports 'word2vec' or 'fasttext'.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    logger.info(\"üìÇ Loading processed data...\")\n",
    "    train_df = pd.read_pickle('../data/processed/train.pkl')\n",
    "    test_df = pd.read_pickle('../data/processed/test.pkl')\n",
    "\n",
    "    # Prepare sentences (list of token lists) for embedding training\n",
    "    sentences = train_df[f'tokens_{variant}'].tolist()\n",
    "\n",
    "    # Train embedding model\n",
    "    logger.info(f\"üß† Training {embedding_type.upper()} model on {variant} tokens...\")\n",
    "    if embedding_type == 'word2vec':\n",
    "        model = Word2Vec(sentences, vector_size=embedding_size, window=5, min_count=1, workers=4, epochs=epochs)\n",
    "    elif embedding_type == 'fasttext':\n",
    "        model = FastText(sentences, vector_size=embedding_size, window=5, min_count=1, workers=4, epochs=epochs)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embedding_type. Use 'word2vec' or 'fasttext'.\")\n",
    "\n",
    "    # Save embedding model\n",
    "    model_path = f'../models/{embedding_type}_{variant}.model'\n",
    "    model.save(model_path)\n",
    "    logger.info(f\"‚úÖ {embedding_type.upper()} model saved to {model_path}\")\n",
    "\n",
    "    # Generate embeddings for train and test\n",
    "    logger.info(\"üìù Generating embeddings for train...\")\n",
    "    X_train_emb = prepare_embeddings(train_df, variant, model, embedding_size)\n",
    "    \n",
    "    logger.info(\"üìù Generating embeddings for test...\")\n",
    "    X_test_emb = prepare_embeddings(test_df, variant, model, embedding_size)\n",
    "\n",
    "    # Results storage\n",
    "    results_binary = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}\n",
    "    results_binary_balanced = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}\n",
    "\n",
    "    # Binary dimensions training\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        logger.info(f\"üß† Training binary model for {dim} with SMOTE...\")\n",
    "        y_train_bin = train_df[dim]\n",
    "        y_test_bin = test_df[dim]\n",
    "\n",
    "        # Apply SMOTE to train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_emb, y_train_bin)\n",
    "\n",
    "        # Train logistic regression\n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Predict on original test\n",
    "        y_pred_bin = clf.predict(X_test_emb)\n",
    "        acc_bin = accuracy_score(y_test_bin, y_pred_bin)\n",
    "        f1_bin = f1_score(y_test_bin, y_pred_bin, average='weighted')\n",
    "        results_binary[dim] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
    "        \n",
    "        print(f\"{dim} - Accuracy: {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
    "        print(classification_report(y_test_bin, y_pred_bin))\n",
    "\n",
    "        # Save model\n",
    "        clf_path = f'../models/binary_{dim}_{embedding_type}_{variant}.pkl'\n",
    "        joblib.dump(clf, clf_path)\n",
    "\n",
    "    # Balanced test evaluation\n",
    "    logger.info(\"\\nüîç Evaluating on SMOTE-balanced test...\")\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        y_test_bin = test_df[dim]\n",
    "        \n",
    "        # Load classifier (since we saved it)\n",
    "        clf = joblib.load(f'../models/binary_{dim}_{embedding_type}_{variant}.pkl')\n",
    "        \n",
    "        # Apply SMOTE to test embeddings\n",
    "        smote_test = SMOTE(random_state=42)\n",
    "        X_test_res, y_test_res = smote_test.fit_resample(X_test_emb, y_test_bin)\n",
    "        \n",
    "        # Predict on balanced test\n",
    "        y_pred_bin = clf.predict(X_test_res)\n",
    "        \n",
    "        acc_bin = accuracy_score(y_test_res, y_pred_bin)\n",
    "        f1_bin = f1_score(y_test_res, y_pred_bin, average='weighted')\n",
    "        \n",
    "        results_binary_balanced[dim] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
    "        print(f\"{dim} - Accuracy (on SMOTE-balanced test): {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
    "        print(classification_report(y_test_res, y_pred_bin))\n",
    "\n",
    "    # Compare results\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        print(f\"\\nüìä Binary {dim} Results:\")\n",
    "        print(f\"Original Test: Accuracy={results_binary[dim]['accuracy']:.4f}, F1={results_binary[dim]['f1']:.4f}\")\n",
    "        print(f\"Balanced Test: Accuracy={results_binary_balanced[dim]['accuracy']:.4f}, F1={results_binary_balanced[dim]['f1']:.4f}\")\n",
    "\n",
    "    logger.info(\"‚úÖ Training and evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train with Word2Vec\n",
    "    train_and_evaluate(embedding_type='word2vec', variant='with_lemma', embedding_size=100, epochs=10)\n",
    "    \n",
    "    # Train with FastText\n",
    "    train_and_evaluate(embedding_type='fasttext', variant='with_lemma', embedding_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1bc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_miniproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
