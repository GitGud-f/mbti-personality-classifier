{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d1da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36d5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to average word embeddings for a list of tokens\n",
    "def average_embedding(tokens, model, embedding_size=100):\n",
    "    \"\"\"\n",
    "    Compute average embedding for a list of tokens.\n",
    "    Ignores words not in vocabulary.\n",
    "    \"\"\"\n",
    "    valid_embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            valid_embeddings.append(model.wv[token])\n",
    "    if not valid_embeddings:\n",
    "        return np.zeros(embedding_size)\n",
    "    return np.mean(valid_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4691bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to prepare embeddings for a dataframe column\n",
    "def prepare_embeddings(df, variant, model, embedding_size=100):\n",
    "    \"\"\"\n",
    "    Generate averaged embeddings for each row's tokens.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for tokens in tqdm(df[f'tokens_{variant}']):\n",
    "        emb = average_embedding(tokens, model, embedding_size)\n",
    "        embeddings.append(emb)\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b27cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training and evaluation function\n",
    "def train_and_evaluate(embedding_type='word2vec', variant='with_lemma', embedding_size=100, epochs=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using specified embeddings.\n",
    "    Supports 'word2vec' or 'fasttext'.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    logger.info(\"üìÇ Loading processed data...\")\n",
    "    train_df = pd.read_pickle('../data/processed/train.pkl')\n",
    "    test_df = pd.read_pickle('../data/processed/test.pkl')\n",
    "\n",
    "    # Prepare sentences (list of token lists) for embedding training\n",
    "    sentences = train_df[f'tokens_{variant}'].tolist()\n",
    "\n",
    "    # Train embedding model\n",
    "    logger.info(f\"üß† Training {embedding_type.upper()} model on {variant} tokens...\")\n",
    "    if embedding_type == 'word2vec':\n",
    "        model = Word2Vec(sentences, vector_size=embedding_size, window=5, min_count=1, workers=4, epochs=epochs)\n",
    "    elif embedding_type == 'fasttext':\n",
    "        model = FastText(sentences, vector_size=embedding_size, window=5, min_count=1, workers=4, epochs=epochs)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embedding_type. Use 'word2vec' or 'fasttext'.\")\n",
    "\n",
    "    # Save embedding model\n",
    "    model_path = f'../models/{embedding_type}_{variant}.model'\n",
    "    model.save(model_path)\n",
    "    logger.info(f\"‚úÖ {embedding_type.upper()} model saved to {model_path}\")\n",
    "\n",
    "    # Generate embeddings for train and test\n",
    "    logger.info(\"üìù Generating embeddings for train...\")\n",
    "    X_train_emb = prepare_embeddings(train_df, variant, model, embedding_size)\n",
    "    \n",
    "    logger.info(\"üìù Generating embeddings for test...\")\n",
    "    X_test_emb = prepare_embeddings(test_df, variant, model, embedding_size)\n",
    "\n",
    "    # Results storage\n",
    "    results_binary = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}\n",
    "    results_binary_balanced = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}\n",
    "\n",
    "    # Binary dimensions training\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        logger.info(f\"üß† Training binary model for {dim} with SMOTE...\")\n",
    "        y_train_bin = train_df[dim]\n",
    "        y_test_bin = test_df[dim]\n",
    "\n",
    "        # Apply SMOTE to train\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_emb, y_train_bin)\n",
    "\n",
    "        # Train logistic regression\n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "        # Predict on original test\n",
    "        y_pred_bin = clf.predict(X_test_emb)\n",
    "        acc_bin = accuracy_score(y_test_bin, y_pred_bin)\n",
    "        f1_bin = f1_score(y_test_bin, y_pred_bin, average='weighted')\n",
    "        results_binary[dim] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
    "        \n",
    "        print(f\"{dim} - Accuracy: {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
    "        print(classification_report(y_test_bin, y_pred_bin))\n",
    "\n",
    "        # Save model\n",
    "        clf_path = f'../models/binary_{dim}_{embedding_type}_{variant}.pkl'\n",
    "        joblib.dump(clf, clf_path)\n",
    "\n",
    "    # Balanced test evaluation\n",
    "    logger.info(\"\\nüîç Evaluating on SMOTE-balanced test...\")\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        y_test_bin = test_df[dim]\n",
    "        \n",
    "        # Load classifier (since we saved it)\n",
    "        clf = joblib.load(f'../models/binary_{dim}_{embedding_type}_{variant}.pkl')\n",
    "        \n",
    "        # Apply SMOTE to test embeddings\n",
    "        smote_test = SMOTE(random_state=42)\n",
    "        X_test_res, y_test_res = smote_test.fit_resample(X_test_emb, y_test_bin)\n",
    "        \n",
    "        # Predict on balanced test\n",
    "        y_pred_bin = clf.predict(X_test_res)\n",
    "        \n",
    "        acc_bin = accuracy_score(y_test_res, y_pred_bin)\n",
    "        f1_bin = f1_score(y_test_res, y_pred_bin, average='weighted')\n",
    "        \n",
    "        results_binary_balanced[dim] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
    "        print(f\"{dim} - Accuracy (on SMOTE-balanced test): {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
    "        print(classification_report(y_test_res, y_pred_bin))\n",
    "\n",
    "    # Compare results\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        print(f\"\\nüìä Binary {dim} Results:\")\n",
    "        print(f\"Original Test: Accuracy={results_binary[dim]['accuracy']:.4f}, F1={results_binary[dim]['f1']:.4f}\")\n",
    "        print(f\"Balanced Test: Accuracy={results_binary_balanced[dim]['accuracy']:.4f}, F1={results_binary_balanced[dim]['f1']:.4f}\")\n",
    "\n",
    "    logger.info(\"‚úÖ Training and evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9de738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:36:16,624 - INFO - üìÇ Loading processed data...\n",
      "2025-11-29 09:36:36,847 - INFO - üß† Training WORD2VEC model on with_lemma tokens...\n",
      "2025-11-29 09:36:36,901 - INFO - collecting all words and their counts\n",
      "2025-11-29 09:36:36,903 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-11-29 09:36:37,996 - INFO - collected 82324 word types from a corpus of 4133474 raw words and 6940 sentences\n",
      "2025-11-29 09:36:37,997 - INFO - Creating a fresh vocabulary\n",
      "2025-11-29 09:36:38,392 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 82324 unique words (100.00% of original 82324, drops 0)', 'datetime': '2025-11-29T09:36:38.391933', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-11-29 09:36:38,395 - INFO - Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 4133474 word corpus (100.00% of original 4133474, drops 0)', 'datetime': '2025-11-29T09:36:38.395195', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-11-29 09:36:39,037 - INFO - deleting the raw counts dictionary of 82324 items\n",
      "2025-11-29 09:36:39,042 - INFO - sample=0.001 downsamples 40 most-common words\n",
      "2025-11-29 09:36:39,044 - INFO - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3844615.7404072164 word corpus (93.0%% of prior 4133474)', 'datetime': '2025-11-29T09:36:39.044715', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-11-29 09:36:40,120 - INFO - estimated required memory for 82324 words and 100 dimensions: 107021200 bytes\n",
      "2025-11-29 09:36:40,121 - INFO - resetting layer weights\n",
      "2025-11-29 09:36:40,191 - INFO - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-11-29T09:36:40.191275', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2025-11-29 09:36:40,192 - INFO - Word2Vec lifecycle event {'msg': 'training model with 4 workers on 82324 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-11-29T09:36:40.192245', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2025-11-29 09:36:41,206 - INFO - EPOCH 0 - PROGRESS: at 28.31% examples, 1074283 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:42,211 - INFO - EPOCH 0 - PROGRESS: at 66.30% examples, 1263430 words/s, in_qsize 8, out_qsize 1\n",
      "2025-11-29 09:36:43,212 - INFO - EPOCH 0 - PROGRESS: at 94.70% examples, 1207374 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:43,401 - INFO - EPOCH 0: training on 4133474 raw words (3844130 effective words) took 3.2s, 1200109 effective words/s\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:36:44,413 - INFO - EPOCH 1 - PROGRESS: at 29.28% examples, 1113184 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:45,414 - INFO - EPOCH 1 - PROGRESS: at 62.77% examples, 1199937 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:46,428 - INFO - EPOCH 1 - PROGRESS: at 95.42% examples, 1213943 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:46,584 - INFO - EPOCH 1: training on 4133474 raw words (3844281 effective words) took 3.2s, 1210418 effective words/s\n",
      "2025-11-29 09:36:47,592 - INFO - EPOCH 2 - PROGRESS: at 35.62% examples, 1358535 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:48,596 - INFO - EPOCH 2 - PROGRESS: at 72.82% examples, 1392701 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:49,343 - INFO - EPOCH 2: training on 4133474 raw words (3844845 effective words) took 2.8s, 1395830 effective words/s\n",
      "2025-11-29 09:36:50,353 - INFO - EPOCH 3 - PROGRESS: at 38.79% examples, 1477493 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:36:51,364 - INFO - EPOCH 3 - PROGRESS: at 77.77% examples, 1480185 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:51,929 - INFO - EPOCH 3: training on 4133474 raw words (3844083 effective words) took 2.6s, 1488650 effective words/s\n",
      "2025-11-29 09:36:52,935 - INFO - EPOCH 4 - PROGRESS: at 37.64% examples, 1438581 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:53,941 - INFO - EPOCH 4 - PROGRESS: at 73.05% examples, 1396339 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:54,641 - INFO - EPOCH 4: training on 4133474 raw words (3845087 effective words) took 2.7s, 1419246 effective words/s\n",
      "2025-11-29 09:36:55,645 - INFO - EPOCH 5 - PROGRESS: at 38.56% examples, 1478181 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:56,651 - INFO - EPOCH 5 - PROGRESS: at 78.23% examples, 1497837 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:36:57,224 - INFO - EPOCH 5: training on 4133474 raw words (3844470 effective words) took 2.6s, 1490524 effective words/s\n",
      "2025-11-29 09:36:58,228 - INFO - EPOCH 6 - PROGRESS: at 29.48% examples, 1126840 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:36:59,231 - INFO - EPOCH 6 - PROGRESS: at 60.42% examples, 1157518 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:37:00,258 - INFO - EPOCH 6 - PROGRESS: at 91.35% examples, 1159993 words/s, in_qsize 5, out_qsize 2\n",
      "2025-11-29 09:37:00,552 - INFO - EPOCH 6: training on 4133474 raw words (3844480 effective words) took 3.3s, 1156408 effective words/s\n",
      "2025-11-29 09:37:01,559 - INFO - EPOCH 7 - PROGRESS: at 33.78% examples, 1286007 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:37:02,559 - INFO - EPOCH 7 - PROGRESS: at 71.18% examples, 1364076 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:37:03,374 - INFO - EPOCH 7: training on 4133474 raw words (3844531 effective words) took 2.8s, 1364080 effective words/s\n",
      "2025-11-29 09:37:04,386 - INFO - EPOCH 8 - PROGRESS: at 34.93% examples, 1323878 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:37:05,391 - INFO - EPOCH 8 - PROGRESS: at 74.03% examples, 1411657 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:37:06,102 - INFO - EPOCH 8: training on 4133474 raw words (3844774 effective words) took 2.7s, 1411504 effective words/s\n",
      "2025-11-29 09:37:07,108 - INFO - EPOCH 9 - PROGRESS: at 37.19% examples, 1423767 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:37:08,112 - INFO - EPOCH 9 - PROGRESS: at 71.66% examples, 1371770 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:37:09,117 - INFO - EPOCH 9 - PROGRESS: at 98.50% examples, 1256721 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:37:09,158 - INFO - EPOCH 9: training on 4133474 raw words (3844248 effective words) took 3.1s, 1259815 effective words/s\n",
      "2025-11-29 09:37:09,159 - INFO - Word2Vec lifecycle event {'msg': 'training on 41334740 raw words (38444929 effective words) took 29.0s, 1327296 effective words/s', 'datetime': '2025-11-29T09:37:09.159511', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2025-11-29 09:37:09,160 - INFO - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=82324, vector_size=100, alpha=0.025>', 'datetime': '2025-11-29T09:37:09.160450', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-11-29 09:37:09,161 - INFO - Word2Vec lifecycle event {'fname_or_handle': '../models/word2vec_with_lemma.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-11-29T09:37:09.161707', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2025-11-29 09:37:09,163 - INFO - not storing attribute cum_table\n",
      "2025-11-29 09:37:09,441 - INFO - saved ../models/word2vec_with_lemma.model\n",
      "2025-11-29 09:37:09,442 - INFO - ‚úÖ WORD2VEC model saved to ../models/word2vec_with_lemma.model\n",
      "2025-11-29 09:37:09,443 - INFO - üìù Generating embeddings for train...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6940/6940 [00:12<00:00, 576.63it/s]\n",
      "2025-11-29 09:37:21,605 - INFO - üìù Generating embeddings for test...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1735/1735 [00:02<00:00, 585.76it/s]\n",
      "2025-11-29 09:37:24,576 - INFO - üß† Training binary model for IE with SMOTE...\n",
      "2025-11-29 09:37:25,359 - INFO - üß† Training binary model for NS with SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE - Accuracy: 0.6870, F1: 0.7091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.67      0.50       401\n",
      "           1       0.87      0.69      0.77      1334\n",
      "\n",
      "    accuracy                           0.69      1735\n",
      "   macro avg       0.64      0.68      0.64      1735\n",
      "weighted avg       0.76      0.69      0.71      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:37:25,591 - INFO - üß† Training binary model for FT with SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NS - Accuracy: 0.6824, F1: 0.7298\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.66      0.37       240\n",
      "           1       0.93      0.69      0.79      1495\n",
      "\n",
      "    accuracy                           0.68      1735\n",
      "   macro avg       0.59      0.67      0.58      1735\n",
      "weighted avg       0.83      0.68      0.73      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:37:25,811 - INFO - üß† Training binary model for JP with SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT - Accuracy: 0.7550, F1: 0.7554\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       796\n",
      "           1       0.79      0.75      0.77       939\n",
      "\n",
      "    accuracy                           0.76      1735\n",
      "   macro avg       0.75      0.76      0.75      1735\n",
      "weighted avg       0.76      0.76      0.76      1735\n",
      "\n",
      "JP - Accuracy: 0.6104, F1: 0.6145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:37:26,003 - INFO - \n",
      "üîç Evaluating on SMOTE-balanced test...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.61      0.65      1048\n",
      "           1       0.51      0.61      0.56       687\n",
      "\n",
      "    accuracy                           0.61      1735\n",
      "   macro avg       0.61      0.61      0.60      1735\n",
      "weighted avg       0.63      0.61      0.61      1735\n",
      "\n",
      "IE - Accuracy (on SMOTE-balanced test): 0.6960, F1: 0.6960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70      1334\n",
      "           1       0.70      0.69      0.69      1334\n",
      "\n",
      "    accuracy                           0.70      2668\n",
      "   macro avg       0.70      0.70      0.70      2668\n",
      "weighted avg       0.70      0.70      0.70      2668\n",
      "\n",
      "NS - Accuracy (on SMOTE-balanced test): 0.6729, F1: 0.6729\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      1495\n",
      "           1       0.67      0.69      0.68      1495\n",
      "\n",
      "    accuracy                           0.67      2990\n",
      "   macro avg       0.67      0.67      0.67      2990\n",
      "weighted avg       0.67      0.67      0.67      2990\n",
      "\n",
      "FT - Accuracy (on SMOTE-balanced test): 0.7614, F1: 0.7614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       939\n",
      "           1       0.77      0.75      0.76       939\n",
      "\n",
      "    accuracy                           0.76      1878\n",
      "   macro avg       0.76      0.76      0.76      1878\n",
      "weighted avg       0.76      0.76      0.76      1878\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:37:26,229 - INFO - ‚úÖ Training and evaluation complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JP - Accuracy (on SMOTE-balanced test): 0.6169, F1: 0.6169\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.61      0.61      1048\n",
      "           1       0.61      0.63      0.62      1048\n",
      "\n",
      "    accuracy                           0.62      2096\n",
      "   macro avg       0.62      0.62      0.62      2096\n",
      "weighted avg       0.62      0.62      0.62      2096\n",
      "\n",
      "\n",
      "üìä Binary IE Results:\n",
      "Original Test: Accuracy=0.6870, F1=0.7091\n",
      "Balanced Test: Accuracy=0.6960, F1=0.6960\n",
      "\n",
      "üìä Binary NS Results:\n",
      "Original Test: Accuracy=0.6824, F1=0.7298\n",
      "Balanced Test: Accuracy=0.6729, F1=0.6729\n",
      "\n",
      "üìä Binary FT Results:\n",
      "Original Test: Accuracy=0.7550, F1=0.7554\n",
      "Balanced Test: Accuracy=0.7614, F1=0.7614\n",
      "\n",
      "üìä Binary JP Results:\n",
      "Original Test: Accuracy=0.6104, F1=0.6145\n",
      "Balanced Test: Accuracy=0.6169, F1=0.6169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:37:33,826 - INFO - üìÇ Loading processed data...\n",
      "2025-11-29 09:37:57,574 - INFO - üß† Training FASTTEXT model on with_lemma tokens...\n",
      "2025-11-29 09:37:57,579 - INFO - collecting all words and their counts\n",
      "2025-11-29 09:37:57,579 - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-11-29 09:37:58,676 - INFO - collected 82324 word types from a corpus of 4133474 raw words and 6940 sentences\n",
      "2025-11-29 09:37:58,677 - INFO - Creating a fresh vocabulary\n",
      "2025-11-29 09:37:59,134 - INFO - FastText lifecycle event {'msg': 'effective_min_count=1 retains 82324 unique words (100.00% of original 82324, drops 0)', 'datetime': '2025-11-29T09:37:59.134285', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-11-29 09:37:59,135 - INFO - FastText lifecycle event {'msg': 'effective_min_count=1 leaves 4133474 word corpus (100.00% of original 4133474, drops 0)', 'datetime': '2025-11-29T09:37:59.135559', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-11-29 09:37:59,733 - INFO - deleting the raw counts dictionary of 82324 items\n",
      "2025-11-29 09:37:59,736 - INFO - sample=0.001 downsamples 40 most-common words\n",
      "2025-11-29 09:37:59,738 - INFO - FastText lifecycle event {'msg': 'downsampling leaves estimated 3844615.7404072164 word corpus (93.0%% of prior 4133474)', 'datetime': '2025-11-29T09:37:59.738499', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-11-29 09:38:01,565 - INFO - estimated required memory for 82324 words, 2000000 buckets and 100 dimensions: 923579012 bytes\n",
      "2025-11-29 09:38:01,566 - INFO - resetting layer weights\n",
      "2025-11-29 09:38:08,391 - INFO - FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-11-29T09:38:08.391563', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2025-11-29 09:38:08,392 - INFO - FastText lifecycle event {'msg': 'training model with 4 workers on 82324 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-11-29T09:38:08.392734', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2025-11-29 09:38:09,451 - INFO - EPOCH 0 - PROGRESS: at 8.62% examples, 315716 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:10,494 - INFO - EPOCH 0 - PROGRESS: at 18.24% examples, 331206 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:11,551 - INFO - EPOCH 0 - PROGRESS: at 27.67% examples, 335319 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:12,580 - INFO - EPOCH 0 - PROGRESS: at 36.95% examples, 338617 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:13,618 - INFO - EPOCH 0 - PROGRESS: at 46.27% examples, 340375 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:14,644 - INFO - EPOCH 0 - PROGRESS: at 55.45% examples, 340958 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:15,666 - INFO - EPOCH 0 - PROGRESS: at 64.39% examples, 339966 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:16,673 - INFO - EPOCH 0 - PROGRESS: at 73.54% examples, 341259 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:17,713 - INFO - EPOCH 0 - PROGRESS: at 82.77% examples, 341664 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:38:18,738 - INFO - EPOCH 0 - PROGRESS: at 92.12% examples, 342492 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:19,593 - INFO - EPOCH 0: training on 4133474 raw words (3844291 effective words) took 11.2s, 343402 effective words/s\n",
      "2025-11-29 09:38:20,635 - INFO - EPOCH 1 - PROGRESS: at 8.65% examples, 320498 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:21,635 - INFO - EPOCH 1 - PROGRESS: at 17.98% examples, 335983 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:22,698 - INFO - EPOCH 1 - PROGRESS: at 27.44% examples, 337666 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:23,736 - INFO - EPOCH 1 - PROGRESS: at 36.73% examples, 339770 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:24,789 - INFO - EPOCH 1 - PROGRESS: at 46.05% examples, 340383 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:25,806 - INFO - EPOCH 1 - PROGRESS: at 55.22% examples, 341450 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:26,857 - INFO - EPOCH 1 - PROGRESS: at 64.64% examples, 341515 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:38:27,897 - INFO - EPOCH 1 - PROGRESS: at 74.03% examples, 342313 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:28,920 - INFO - EPOCH 1 - PROGRESS: at 83.27% examples, 343248 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:29,941 - INFO - EPOCH 1 - PROGRESS: at 92.35% examples, 343185 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:38:30,772 - INFO - EPOCH 1: training on 4133474 raw words (3844200 effective words) took 11.2s, 343967 effective words/s\n",
      "2025-11-29 09:38:31,830 - INFO - EPOCH 2 - PROGRESS: at 8.62% examples, 315106 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:32,858 - INFO - EPOCH 2 - PROGRESS: at 18.24% examples, 333213 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:33,896 - INFO - EPOCH 2 - PROGRESS: at 27.67% examples, 338601 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:34,908 - INFO - EPOCH 2 - PROGRESS: at 36.28% examples, 336060 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:35,921 - INFO - EPOCH 2 - PROGRESS: at 45.35% examples, 338244 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:36,935 - INFO - EPOCH 2 - PROGRESS: at 54.51% examples, 339786 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:37,957 - INFO - EPOCH 2 - PROGRESS: at 63.92% examples, 341581 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:38,959 - INFO - EPOCH 2 - PROGRESS: at 72.82% examples, 341730 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:38:39,987 - INFO - EPOCH 2 - PROGRESS: at 82.12% examples, 342671 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:41,008 - INFO - EPOCH 2 - PROGRESS: at 91.38% examples, 343558 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:41,930 - INFO - EPOCH 2: training on 4133474 raw words (3845299 effective words) took 11.2s, 344714 effective words/s\n",
      "2025-11-29 09:38:42,979 - INFO - EPOCH 3 - PROGRESS: at 8.65% examples, 318125 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:44,003 - INFO - EPOCH 3 - PROGRESS: at 18.24% examples, 335301 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:45,014 - INFO - EPOCH 3 - PROGRESS: at 27.44% examples, 340007 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:46,052 - INFO - EPOCH 3 - PROGRESS: at 36.73% examples, 341484 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:47,091 - INFO - EPOCH 3 - PROGRESS: at 46.05% examples, 342748 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:48,106 - INFO - EPOCH 3 - PROGRESS: at 55.22% examples, 343482 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:49,142 - INFO - EPOCH 3 - PROGRESS: at 64.64% examples, 343948 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:50,169 - INFO - EPOCH 3 - PROGRESS: at 74.03% examples, 345011 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:51,173 - INFO - EPOCH 3 - PROGRESS: at 83.03% examples, 345339 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:52,184 - INFO - EPOCH 3 - PROGRESS: at 92.12% examples, 345405 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:53,062 - INFO - EPOCH 3: training on 4133474 raw words (3843630 effective words) took 11.1s, 345365 effective words/s\n",
      "2025-11-29 09:38:54,125 - INFO - EPOCH 4 - PROGRESS: at 8.65% examples, 314441 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:55,155 - INFO - EPOCH 4 - PROGRESS: at 18.24% examples, 332498 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:56,160 - INFO - EPOCH 4 - PROGRESS: at 26.99% examples, 332864 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:38:57,209 - INFO - EPOCH 4 - PROGRESS: at 36.07% examples, 333188 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:58,212 - INFO - EPOCH 4 - PROGRESS: at 45.12% examples, 336559 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:38:59,252 - INFO - EPOCH 4 - PROGRESS: at 54.51% examples, 338435 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:39:00,258 - INFO - EPOCH 4 - PROGRESS: at 63.67% examples, 339862 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:01,307 - INFO - EPOCH 4 - PROGRESS: at 72.82% examples, 339422 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:02,316 - INFO - EPOCH 4 - PROGRESS: at 81.90% examples, 340294 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:03,336 - INFO - EPOCH 4 - PROGRESS: at 90.86% examples, 340574 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:04,328 - INFO - EPOCH 4: training on 4133474 raw words (3845188 effective words) took 11.3s, 341439 effective words/s\n",
      "2025-11-29 09:39:05,380 - INFO - EPOCH 5 - PROGRESS: at 8.62% examples, 317892 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:06,410 - INFO - EPOCH 5 - PROGRESS: at 18.24% examples, 334361 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:39:07,438 - INFO - EPOCH 5 - PROGRESS: at 27.44% examples, 337593 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:08,443 - INFO - EPOCH 5 - PROGRESS: at 36.50% examples, 340215 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:09,447 - INFO - EPOCH 5 - PROGRESS: at 45.35% examples, 340478 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:10,449 - INFO - EPOCH 5 - PROGRESS: at 54.28% examples, 340829 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:39:11,455 - INFO - EPOCH 5 - PROGRESS: at 63.44% examples, 342018 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:12,458 - INFO - EPOCH 5 - PROGRESS: at 72.61% examples, 343191 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:13,514 - INFO - EPOCH 5 - PROGRESS: at 81.90% examples, 342837 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:14,538 - INFO - EPOCH 5 - PROGRESS: at 91.12% examples, 343554 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:39:15,481 - INFO - EPOCH 5: training on 4133474 raw words (3844684 effective words) took 11.1s, 344901 effective words/s\n",
      "2025-11-29 09:39:16,518 - INFO - EPOCH 6 - PROGRESS: at 8.65% examples, 321847 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:17,543 - INFO - EPOCH 6 - PROGRESS: at 18.24% examples, 337098 words/s, in_qsize 7, out_qsize 0\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "2025-11-29 09:39:18,579 - INFO - EPOCH 6 - PROGRESS: at 27.67% examples, 341375 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:19,600 - INFO - EPOCH 6 - PROGRESS: at 36.95% examples, 343886 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:39:20,622 - INFO - EPOCH 6 - PROGRESS: at 46.05% examples, 344023 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:21,654 - INFO - EPOCH 6 - PROGRESS: at 55.45% examples, 345123 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:22,690 - INFO - EPOCH 6 - PROGRESS: at 64.39% examples, 342812 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:23,725 - INFO - EPOCH 6 - PROGRESS: at 73.78% examples, 343658 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:24,749 - INFO - EPOCH 6 - PROGRESS: at 83.03% examples, 344421 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:39:25,774 - INFO - EPOCH 6 - PROGRESS: at 92.35% examples, 344994 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:39:26,596 - INFO - EPOCH 6: training on 4133474 raw words (3844393 effective words) took 11.1s, 345943 effective words/s\n",
      "2025-11-29 09:39:27,636 - INFO - EPOCH 7 - PROGRESS: at 8.65% examples, 320973 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:28,667 - INFO - EPOCH 7 - PROGRESS: at 18.24% examples, 335829 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:29,687 - INFO - EPOCH 7 - PROGRESS: at 27.44% examples, 339233 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:30,722 - INFO - EPOCH 7 - PROGRESS: at 36.73% examples, 341189 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:31,768 - INFO - EPOCH 7 - PROGRESS: at 46.05% examples, 342008 words/s, in_qsize 6, out_qsize 1\n",
      "2025-11-29 09:39:32,769 - INFO - EPOCH 7 - PROGRESS: at 55.22% examples, 343654 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:33,817 - INFO - EPOCH 7 - PROGRESS: at 64.64% examples, 343536 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:39:34,857 - INFO - EPOCH 7 - PROGRESS: at 74.03% examples, 344112 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:35,884 - INFO - EPOCH 7 - PROGRESS: at 83.27% examples, 344711 words/s, in_qsize 8, out_qsize 0\n",
      "2025-11-29 09:39:36,919 - INFO - EPOCH 7 - PROGRESS: at 92.35% examples, 344055 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:37,752 - INFO - EPOCH 7: training on 4133474 raw words (3844586 effective words) took 11.2s, 344704 effective words/s\n",
      "2025-11-29 09:39:38,833 - INFO - EPOCH 8 - PROGRESS: at 8.65% examples, 309155 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:39,870 - INFO - EPOCH 8 - PROGRESS: at 18.24% examples, 328418 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:40,907 - INFO - EPOCH 8 - PROGRESS: at 27.67% examples, 335459 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:41,946 - INFO - EPOCH 8 - PROGRESS: at 36.95% examples, 337922 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:42,972 - INFO - EPOCH 8 - PROGRESS: at 46.27% examples, 340583 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:43,975 - INFO - EPOCH 8 - PROGRESS: at 55.45% examples, 342434 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:44,997 - INFO - EPOCH 8 - PROGRESS: at 64.87% examples, 343732 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:46,011 - INFO - EPOCH 8 - PROGRESS: at 74.03% examples, 344340 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:47,043 - INFO - EPOCH 8 - PROGRESS: at 83.27% examples, 344702 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:48,075 - INFO - EPOCH 8 - PROGRESS: at 92.58% examples, 345012 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:48,873 - INFO - EPOCH 8: training on 4133474 raw words (3845172 effective words) took 11.1s, 345848 effective words/s\n",
      "2025-11-29 09:39:49,909 - INFO - EPOCH 9 - PROGRESS: at 8.65% examples, 322318 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:50,915 - INFO - EPOCH 9 - PROGRESS: at 17.98% examples, 336210 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:51,973 - INFO - EPOCH 9 - PROGRESS: at 27.44% examples, 338371 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:53,029 - INFO - EPOCH 9 - PROGRESS: at 36.73% examples, 338686 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:54,083 - INFO - EPOCH 9 - PROGRESS: at 46.05% examples, 339501 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:55,090 - INFO - EPOCH 9 - PROGRESS: at 55.22% examples, 341261 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:56,138 - INFO - EPOCH 9 - PROGRESS: at 64.64% examples, 341528 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:57,177 - INFO - EPOCH 9 - PROGRESS: at 74.03% examples, 342386 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:58,203 - INFO - EPOCH 9 - PROGRESS: at 83.27% examples, 343199 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:39:59,221 - INFO - EPOCH 9 - PROGRESS: at 92.35% examples, 343251 words/s, in_qsize 7, out_qsize 0\n",
      "2025-11-29 09:40:00,057 - INFO - EPOCH 9: training on 4133474 raw words (3845030 effective words) took 11.2s, 343881 effective words/s\n",
      "2025-11-29 09:40:00,058 - INFO - FastText lifecycle event {'msg': 'training on 41334740 raw words (38446473 effective words) took 111.7s, 344311 effective words/s', 'datetime': '2025-11-29T09:40:00.058742', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2025-11-29 09:40:03,837 - INFO - FastText lifecycle event {'params': 'FastText<vocab=82324, vector_size=100, alpha=0.025>', 'datetime': '2025-11-29T09:40:03.837949', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-11-29 09:40:03,839 - INFO - FastText lifecycle event {'fname_or_handle': '../models/fasttext_with_lemma.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-11-29T09:40:03.839006', 'gensim': '4.4.0', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:28:27) [Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2025-11-29 09:40:03,840 - INFO - storing np array 'vectors_ngrams' to ../models/fasttext_with_lemma.model.wv.vectors_ngrams.npy\n",
      "2025-11-29 09:40:05,365 - INFO - not storing attribute buckets_word\n",
      "2025-11-29 09:40:05,366 - INFO - not storing attribute vectors\n",
      "2025-11-29 09:40:05,367 - INFO - not storing attribute cum_table\n",
      "2025-11-29 09:40:05,563 - INFO - saved ../models/fasttext_with_lemma.model\n",
      "2025-11-29 09:40:05,564 - INFO - ‚úÖ FASTTEXT model saved to ../models/fasttext_with_lemma.model\n",
      "2025-11-29 09:40:05,564 - INFO - üìù Generating embeddings for train...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6940/6940 [00:10<00:00, 633.52it/s]\n",
      "2025-11-29 09:40:16,533 - INFO - üìù Generating embeddings for test...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1735/1735 [00:03<00:00, 523.41it/s]\n",
      "2025-11-29 09:40:19,853 - INFO - üß† Training binary model for IE with SMOTE...\n",
      "2025-11-29 09:40:20,041 - INFO - üß† Training binary model for NS with SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE - Accuracy: 0.6755, F1: 0.6990\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.67      0.49       401\n",
      "           1       0.87      0.68      0.76      1334\n",
      "\n",
      "    accuracy                           0.68      1735\n",
      "   macro avg       0.63      0.67      0.63      1735\n",
      "weighted avg       0.76      0.68      0.70      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:40:20,335 - INFO - üß† Training binary model for FT with SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NS - Accuracy: 0.6795, F1: 0.7271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.64      0.36       240\n",
      "           1       0.92      0.69      0.79      1495\n",
      "\n",
      "    accuracy                           0.68      1735\n",
      "   macro avg       0.58      0.66      0.57      1735\n",
      "weighted avg       0.83      0.68      0.73      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:40:20,660 - INFO - üß† Training binary model for JP with SMOTE...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT - Accuracy: 0.7493, F1: 0.7497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.73       796\n",
      "           1       0.78      0.74      0.76       939\n",
      "\n",
      "    accuracy                           0.75      1735\n",
      "   macro avg       0.75      0.75      0.75      1735\n",
      "weighted avg       0.75      0.75      0.75      1735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 09:40:20,879 - INFO - \n",
      "üîç Evaluating on SMOTE-balanced test...\n",
      "2025-11-29 09:40:21,019 - INFO - ‚úÖ Training and evaluation complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JP - Accuracy: 0.6023, F1: 0.6063\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65      1048\n",
      "           1       0.50      0.59      0.54       687\n",
      "\n",
      "    accuracy                           0.60      1735\n",
      "   macro avg       0.60      0.60      0.59      1735\n",
      "weighted avg       0.62      0.60      0.61      1735\n",
      "\n",
      "IE - Accuracy (on SMOTE-balanced test): 0.6822, F1: 0.6822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68      1334\n",
      "           1       0.68      0.68      0.68      1334\n",
      "\n",
      "    accuracy                           0.68      2668\n",
      "   macro avg       0.68      0.68      0.68      2668\n",
      "weighted avg       0.68      0.68      0.68      2668\n",
      "\n",
      "NS - Accuracy (on SMOTE-balanced test): 0.6722, F1: 0.6722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      1495\n",
      "           1       0.67      0.69      0.68      1495\n",
      "\n",
      "    accuracy                           0.67      2990\n",
      "   macro avg       0.67      0.67      0.67      2990\n",
      "weighted avg       0.67      0.67      0.67      2990\n",
      "\n",
      "FT - Accuracy (on SMOTE-balanced test): 0.7529, F1: 0.7529\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76       939\n",
      "           1       0.76      0.74      0.75       939\n",
      "\n",
      "    accuracy                           0.75      1878\n",
      "   macro avg       0.75      0.75      0.75      1878\n",
      "weighted avg       0.75      0.75      0.75      1878\n",
      "\n",
      "JP - Accuracy (on SMOTE-balanced test): 0.6011, F1: 0.6011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60      1048\n",
      "           1       0.60      0.59      0.60      1048\n",
      "\n",
      "    accuracy                           0.60      2096\n",
      "   macro avg       0.60      0.60      0.60      2096\n",
      "weighted avg       0.60      0.60      0.60      2096\n",
      "\n",
      "\n",
      "üìä Binary IE Results:\n",
      "Original Test: Accuracy=0.6755, F1=0.6990\n",
      "Balanced Test: Accuracy=0.6822, F1=0.6822\n",
      "\n",
      "üìä Binary NS Results:\n",
      "Original Test: Accuracy=0.6795, F1=0.7271\n",
      "Balanced Test: Accuracy=0.6722, F1=0.6722\n",
      "\n",
      "üìä Binary FT Results:\n",
      "Original Test: Accuracy=0.7493, F1=0.7497\n",
      "Balanced Test: Accuracy=0.7529, F1=0.7529\n",
      "\n",
      "üìä Binary JP Results:\n",
      "Original Test: Accuracy=0.6023, F1=0.6063\n",
      "Balanced Test: Accuracy=0.6011, F1=0.6011\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train with Word2Vec\n",
    "    train_and_evaluate(embedding_type='word2vec', variant='with_lemma', embedding_size=100, epochs=10)\n",
    "    \n",
    "    # Train with FastText\n",
    "    train_and_evaluate(embedding_type='fasttext', variant='with_lemma', embedding_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1bc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_miniproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
