{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.chunk import RegexpParser\n",
    "import contractions\n",
    "import re\n",
    "import emoji\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cd97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean raw text by removing URLs, emojis, mentions, and MBTI codes.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?\\S+|www\\S+', '', text)\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'@\\w+|#', '', text)\n",
    "    text = re.sub(r\"[^a-z\\']\", ' ', text)\n",
    "    # Remove MBTI type codes (e.g., INFJ, ENTP) to avoid leaking information\n",
    "    text = re.sub(r'\\b(I|E)(N|S)(F|T)(J|P)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Remove common footer\n",
    "    text = re.sub(r'\\bsent (from )?my \\w+(\\s\\w+)? using tapatalk\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'w w w', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_posts(posts_str: str) -> str:\n",
    "    \"\"\"Split multi-post string (separated by '|||'), clean each, and join.\"\"\"\n",
    "    posts = posts_str.split('|||')\n",
    "    cleaned_posts = [clean_text(post) for post in posts]\n",
    "    joined = ' '.join(cleaned_posts)\n",
    "    return re.sub(r'\\s+', ' ', joined).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d845da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding function\n",
    "def encode_mbti_type(mbti: str) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Encode MBTI type as 4 binary dimensions:\n",
    "    I/E â†’ 1/0, N/S â†’ 1/0, F/T â†’ 1/0, J/P â†’ 1/0\n",
    "    \"\"\"\n",
    "    return (\n",
    "        1 if mbti[0] == 'I' else 0,\n",
    "        1 if mbti[1] == 'N' else 0,\n",
    "        1 if mbti[2] == 'F' else 0,\n",
    "        1 if mbti[3] == 'J' else 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e103189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK components\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "WNL = WordNetLemmatizer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    \"\"\"Map POS tag to WordNet format for lemmatization.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization variants\n",
    "def tokens_without_lemma(text: str) -> List[str]:\n",
    "    fixed = contractions.fix(text)\n",
    "    tokens = word_tokenize(fixed)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "    return filtered_tokens\n",
    "\n",
    "def tokens_with_lemma(text: str) -> List[str]:\n",
    "    fixed = contractions.fix(text)\n",
    "    tokens = word_tokenize(fixed)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "    lemmatized = [WNL.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "    return lemmatized\n",
    "\n",
    "def tokens_with_lemma_pos(text: str) -> List[str]:\n",
    "    fixed = contractions.fix(text)\n",
    "    tokens = word_tokenize(fixed)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    lemmatized = [\n",
    "        WNL.lemmatize(token.lower(), pos=get_wordnet_pos(pos))\n",
    "        for token, pos in pos_tags\n",
    "    ]\n",
    "    return lemmatized\n",
    "\n",
    "def tokens_with_spacy_dep(text: str) -> List[str]:\n",
    "    doc = nlp(contractions.fix(text))\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.pos_ == 'VERB':\n",
    "                subj = next((child.text for child in token.children if child.dep_ == 'nsubj'), None)\n",
    "                obj = next((child.text for child in token.children if child.dep_ in ('dobj', 'pobj')), None)\n",
    "                if subj and obj:\n",
    "                    relations.append(f\"{subj}-{token.lemma_}-{obj}\")\n",
    "    return relations\n",
    "\n",
    "def tokens_with_spacy_chunking(text: str) -> List[str]:\n",
    "    doc = nlp(contractions.fix(text))\n",
    "    chunks = [chunk.text for chunk in doc.noun_chunks] + [vp.text for vp in doc if vp.pos_ == 'VERB']\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82380381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram generation\n",
    "def generate_ngrams(tokens: List[str]) -> Tuple[List[Tuple], List[Tuple], List[Tuple]]:\n",
    "    if not tokens:\n",
    "        return [], [], []\n",
    "    unigrams = list(ngrams(tokens, 1))\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "    return unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef36da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:57<00:00, 150.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ·ï¸ Encoding MBTI types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:00<00:00, 1114583.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: without_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:25<00:00, 343.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for without_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:01<00:00, 4443.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:43<00:00, 199.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:01<00:00, 4779.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_lemma_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [04:30<00:00, 32.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_lemma_pos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:01<00:00, 4536.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_dep_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [06:26<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_dep_tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:04<00:00, 1819.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_chunking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [07:21<00:00, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:02<00:00, 3941.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Main processing\n",
    "df = pd.read_csv(\"../data/raw/mbti_1.csv\")  # Adjust path if needed\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning posts...\")\n",
    "tqdm.pandas()\n",
    "df['cleaned_posts'] = df['posts'].progress_apply(preprocess_posts)\n",
    "\n",
    "print(\"ðŸ·ï¸ Encoding MBTI types...\")\n",
    "df[['IE', 'NS', 'FT', 'JP']] = pd.DataFrame(\n",
    "    df['type'].progress_apply(encode_mbti_type).tolist(),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "variants = [\n",
    "    'without_lemma',\n",
    "    'with_lemma',\n",
    "    'with_lemma_pos',\n",
    "    'with_dep_tree',\n",
    "    'with_chunking'\n",
    "]\n",
    "\n",
    "for var in variants:\n",
    "    print(f\"ðŸ”¤ Processing variant: {var}\")\n",
    "    if var == 'without_lemma':\n",
    "        func = tokens_without_lemma\n",
    "    elif var == 'with_lemma':\n",
    "        func = tokens_with_lemma\n",
    "    elif var == 'with_lemma_pos':\n",
    "        func = tokens_with_lemma_pos\n",
    "    elif var == 'with_dep_tree':\n",
    "        func = tokens_with_dependency_tree\n",
    "    elif var == 'with_chunking':\n",
    "        func = tokens_with_chunking\n",
    "    \n",
    "    df[f'tokens_{var}'] = df['cleaned_posts'].progress_apply(func)\n",
    "    \n",
    "    print(f\"ðŸ“Š Generating n-grams for {var}...\")\n",
    "    ngram_results = df[f'tokens_{var}'].progress_apply(generate_ngrams)\n",
    "    df[[f'Unigrams_{var}', f'Bigrams_{var}', f'Trigrams_{var}']] = pd.DataFrame(\n",
    "        ngram_results.tolist(), index=df.index\n",
    "    )\n",
    "\n",
    "# Drop intermediate columns\n",
    "df = df.drop(columns=['posts', 'cleaned_posts'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeaee952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Splitting data into train/test...\n",
      "âœ… Processing complete! Data saved to ../data/processed/train.pkl and test.pkl\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "print(\"âœ‚ï¸ Splitting data into train/test...\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['type'])\n",
    "\n",
    "# Save to ../data/processed\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "train_df.to_pickle('../data/processed/train.pkl')\n",
    "test_df.to_pickle('../data/processed/test.pkl')\n",
    "\n",
    "print(\"âœ… Processing complete! Data saved to ../data/processed/train.pkl and test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58b42ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>FT</th>\n",
       "      <th>JP</th>\n",
       "      <th>tokens_without_lemma</th>\n",
       "      <th>Unigrams_without_lemma</th>\n",
       "      <th>Bigrams_without_lemma</th>\n",
       "      <th>Trigrams_without_lemma</th>\n",
       "      <th>tokens_with_lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>Bigrams_with_lemma_pos</th>\n",
       "      <th>Trigrams_with_lemma_pos</th>\n",
       "      <th>tokens_with_dep_tree</th>\n",
       "      <th>Unigrams_with_dep_tree</th>\n",
       "      <th>Bigrams_with_dep_tree</th>\n",
       "      <th>Trigrams_with_dep_tree</th>\n",
       "      <th>tokens_with_chunking</th>\n",
       "      <th>Unigrams_with_chunking</th>\n",
       "      <th>Bigrams_with_chunking</th>\n",
       "      <th>Trigrams_with_chunking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8331</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['this, actually, exactly, expected, laughing,...</td>\n",
       "      <td>[('this,), (actually,), (exactly,), (expected,...</td>\n",
       "      <td>[('this, actually), (actually, exactly), (exac...</td>\n",
       "      <td>[('this, actually, exactly), (actually, exactl...</td>\n",
       "      <td>['this, actually, exactly, expected, laughing,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[('this, actually), (actually, exactly), (exac...</td>\n",
       "      <td>[('this, actually, exactly), (actually, exactl...</td>\n",
       "      <td>['this-is-i, i-expected-introversion, i-laughi...</td>\n",
       "      <td>[('this-is-i,), (i-expected-introversion,), (i...</td>\n",
       "      <td>[('this-is-i, i-expected-introversion), (i-exp...</td>\n",
       "      <td>[('this-is-i, i-expected-introversion, i-laugh...</td>\n",
       "      <td>['this, is, i, expected, laughing introversion...</td>\n",
       "      <td>[('this,), (is,), (i,), (expected,), (laughing...</td>\n",
       "      <td>[('this, is), (is, i), (i, expected), (expecte...</td>\n",
       "      <td>[('this, is, i), (is, i, expected), (i, expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['nope, ever, busy, work, causes, adrenaline, ...</td>\n",
       "      <td>[('nope,), (ever,), (busy,), (work,), (causes,...</td>\n",
       "      <td>[('nope, ever), (ever, busy), (busy, work), (w...</td>\n",
       "      <td>[('nope, ever, busy), (ever, busy, work), (bus...</td>\n",
       "      <td>['nope, ever, busy, work, cause, adrenaline, r...</td>\n",
       "      <td>...</td>\n",
       "      <td>[('nope, ever), (ever, busy), (busy, work), (w...</td>\n",
       "      <td>[('nope, ever, busy), (ever, busy, work), (bus...</td>\n",
       "      <td>[i-do-anything, anything-make-one, one-keep-fr...</td>\n",
       "      <td>[(i-do-anything,), (anything-make-one,), (one-...</td>\n",
       "      <td>[(i-do-anything, anything-make-one), (anything...</td>\n",
       "      <td>[(i-do-anything, anything-make-one, one-keep-f...</td>\n",
       "      <td>[i, am, busy, with work causes, work causes, a...</td>\n",
       "      <td>[(i,), (am,), (busy,), (with work causes,), (w...</td>\n",
       "      <td>[(i, am), (am, busy), (busy, with work causes)...</td>\n",
       "      <td>[(i, am, busy), (am, busy, with work causes), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['yes, peace, absence, conflict, friend, suxx,...</td>\n",
       "      <td>[('yes,), (peace,), (absence,), (conflict,), (...</td>\n",
       "      <td>[('yes, peace), (peace, absence), (absence, co...</td>\n",
       "      <td>[('yes, peace, absence), (peace, absence, conf...</td>\n",
       "      <td>['yes, peace, absence, conflict, friend, suxx,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[('yes, peace), (peace, absence), (absence, co...</td>\n",
       "      <td>[('yes, peace, absence), (peace, absence, conf...</td>\n",
       "      <td>[peace-is-absence, suxx-hardd-peace, peace-be-...</td>\n",
       "      <td>[(peace-is-absence,), (suxx-hardd-peace,), (pe...</td>\n",
       "      <td>[(peace-is-absence, suxx-hardd-peace), (suxx-h...</td>\n",
       "      <td>[(peace-is-absence, suxx-hardd-peace, peace-be...</td>\n",
       "      <td>['yes peace, is the absence of conflict, the a...</td>\n",
       "      <td>[('yes peace,), (is the absence of conflict,),...</td>\n",
       "      <td>[('yes peace, is the absence of conflict), (is...</td>\n",
       "      <td>[('yes peace, is the absence of conflict, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[', apologize, delayed, response, thank, takin...</td>\n",
       "      <td>[(',), (apologize,), (delayed,), (response,), ...</td>\n",
       "      <td>[(', apologize), (apologize, delayed), (delaye...</td>\n",
       "      <td>[(', apologize, delayed), (apologize, delayed,...</td>\n",
       "      <td>[', apologize, delayed, response, thank, takin...</td>\n",
       "      <td>...</td>\n",
       "      <td>[(', apologize), (apologize, delay), (delay, r...</td>\n",
       "      <td>[(', apologize, delay), (apologize, delay, res...</td>\n",
       "      <td>[i-apologize-response, you-taking-time, it-is-...</td>\n",
       "      <td>[(i-apologize-response,), (you-taking-time,), ...</td>\n",
       "      <td>[(i-apologize-response, you-taking-time), (you...</td>\n",
       "      <td>[(i-apologize-response, you-taking-time, it-is...</td>\n",
       "      <td>[i, apologize for the delayed response, for th...</td>\n",
       "      <td>[(i,), (apologize for the delayed response,), ...</td>\n",
       "      <td>[(i, apologize for the delayed response), (apo...</td>\n",
       "      <td>[(i, apologize for the delayed response, for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['nightglow, even, imagine, must, struggling, ...</td>\n",
       "      <td>[('nightglow,), (even,), (imagine,), (must,), ...</td>\n",
       "      <td>[('nightglow, even), (even, imagine), (imagine...</td>\n",
       "      <td>[('nightglow, even, imagine), (even, imagine, ...</td>\n",
       "      <td>['nightglow, even, imagine, must, struggling, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[('nightglow, even), (even, imagine), (imagine...</td>\n",
       "      <td>[('nightglow, even, imagine), (even, imagine, ...</td>\n",
       "      <td>[i-imagine-you, you-be-right, you-struggling-r...</td>\n",
       "      <td>[(i-imagine-you,), (you-be-right,), (you-strug...</td>\n",
       "      <td>[(i-imagine-you, you-be-right), (you-be-right,...</td>\n",
       "      <td>[(i-imagine-you, you-be-right, you-struggling-...</td>\n",
       "      <td>['nightglow i, i, imagine, be, struggling with...</td>\n",
       "      <td>[('nightglow i,), (i,), (imagine,), (be,), (st...</td>\n",
       "      <td>[('nightglow i, i), (i, imagine), (imagine, be...</td>\n",
       "      <td>[('nightglow i, i, imagine), (i, imagine, be),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      type  IE  NS  FT  JP                               tokens_without_lemma  \\\n",
       "8331  INFP   1   1   1   0  ['this, actually, exactly, expected, laughing,...   \n",
       "1290  ISTP   1   0   0   0  ['nope, ever, busy, work, causes, adrenaline, ...   \n",
       "1982  ENFJ   0   1   1   1  ['yes, peace, absence, conflict, friend, suxx,...   \n",
       "769   INFP   1   1   1   0  [', apologize, delayed, response, thank, takin...   \n",
       "8339  INFP   1   1   1   0  ['nightglow, even, imagine, must, struggling, ...   \n",
       "\n",
       "                                 Unigrams_without_lemma  \\\n",
       "8331  [('this,), (actually,), (exactly,), (expected,...   \n",
       "1290  [('nope,), (ever,), (busy,), (work,), (causes,...   \n",
       "1982  [('yes,), (peace,), (absence,), (conflict,), (...   \n",
       "769   [(',), (apologize,), (delayed,), (response,), ...   \n",
       "8339  [('nightglow,), (even,), (imagine,), (must,), ...   \n",
       "\n",
       "                                  Bigrams_without_lemma  \\\n",
       "8331  [('this, actually), (actually, exactly), (exac...   \n",
       "1290  [('nope, ever), (ever, busy), (busy, work), (w...   \n",
       "1982  [('yes, peace), (peace, absence), (absence, co...   \n",
       "769   [(', apologize), (apologize, delayed), (delaye...   \n",
       "8339  [('nightglow, even), (even, imagine), (imagine...   \n",
       "\n",
       "                                 Trigrams_without_lemma  \\\n",
       "8331  [('this, actually, exactly), (actually, exactl...   \n",
       "1290  [('nope, ever, busy), (ever, busy, work), (bus...   \n",
       "1982  [('yes, peace, absence), (peace, absence, conf...   \n",
       "769   [(', apologize, delayed), (apologize, delayed,...   \n",
       "8339  [('nightglow, even, imagine), (even, imagine, ...   \n",
       "\n",
       "                                      tokens_with_lemma  ...  \\\n",
       "8331  ['this, actually, exactly, expected, laughing,...  ...   \n",
       "1290  ['nope, ever, busy, work, cause, adrenaline, r...  ...   \n",
       "1982  ['yes, peace, absence, conflict, friend, suxx,...  ...   \n",
       "769   [', apologize, delayed, response, thank, takin...  ...   \n",
       "8339  ['nightglow, even, imagine, must, struggling, ...  ...   \n",
       "\n",
       "                                 Bigrams_with_lemma_pos  \\\n",
       "8331  [('this, actually), (actually, exactly), (exac...   \n",
       "1290  [('nope, ever), (ever, busy), (busy, work), (w...   \n",
       "1982  [('yes, peace), (peace, absence), (absence, co...   \n",
       "769   [(', apologize), (apologize, delay), (delay, r...   \n",
       "8339  [('nightglow, even), (even, imagine), (imagine...   \n",
       "\n",
       "                                Trigrams_with_lemma_pos  \\\n",
       "8331  [('this, actually, exactly), (actually, exactl...   \n",
       "1290  [('nope, ever, busy), (ever, busy, work), (bus...   \n",
       "1982  [('yes, peace, absence), (peace, absence, conf...   \n",
       "769   [(', apologize, delay), (apologize, delay, res...   \n",
       "8339  [('nightglow, even, imagine), (even, imagine, ...   \n",
       "\n",
       "                                   tokens_with_dep_tree  \\\n",
       "8331  ['this-is-i, i-expected-introversion, i-laughi...   \n",
       "1290  [i-do-anything, anything-make-one, one-keep-fr...   \n",
       "1982  [peace-is-absence, suxx-hardd-peace, peace-be-...   \n",
       "769   [i-apologize-response, you-taking-time, it-is-...   \n",
       "8339  [i-imagine-you, you-be-right, you-struggling-r...   \n",
       "\n",
       "                                 Unigrams_with_dep_tree  \\\n",
       "8331  [('this-is-i,), (i-expected-introversion,), (i...   \n",
       "1290  [(i-do-anything,), (anything-make-one,), (one-...   \n",
       "1982  [(peace-is-absence,), (suxx-hardd-peace,), (pe...   \n",
       "769   [(i-apologize-response,), (you-taking-time,), ...   \n",
       "8339  [(i-imagine-you,), (you-be-right,), (you-strug...   \n",
       "\n",
       "                                  Bigrams_with_dep_tree  \\\n",
       "8331  [('this-is-i, i-expected-introversion), (i-exp...   \n",
       "1290  [(i-do-anything, anything-make-one), (anything...   \n",
       "1982  [(peace-is-absence, suxx-hardd-peace), (suxx-h...   \n",
       "769   [(i-apologize-response, you-taking-time), (you...   \n",
       "8339  [(i-imagine-you, you-be-right), (you-be-right,...   \n",
       "\n",
       "                                 Trigrams_with_dep_tree  \\\n",
       "8331  [('this-is-i, i-expected-introversion, i-laugh...   \n",
       "1290  [(i-do-anything, anything-make-one, one-keep-f...   \n",
       "1982  [(peace-is-absence, suxx-hardd-peace, peace-be...   \n",
       "769   [(i-apologize-response, you-taking-time, it-is...   \n",
       "8339  [(i-imagine-you, you-be-right, you-struggling-...   \n",
       "\n",
       "                                   tokens_with_chunking  \\\n",
       "8331  ['this, is, i, expected, laughing introversion...   \n",
       "1290  [i, am, busy, with work causes, work causes, a...   \n",
       "1982  ['yes peace, is the absence of conflict, the a...   \n",
       "769   [i, apologize for the delayed response, for th...   \n",
       "8339  ['nightglow i, i, imagine, be, struggling with...   \n",
       "\n",
       "                                 Unigrams_with_chunking  \\\n",
       "8331  [('this,), (is,), (i,), (expected,), (laughing...   \n",
       "1290  [(i,), (am,), (busy,), (with work causes,), (w...   \n",
       "1982  [('yes peace,), (is the absence of conflict,),...   \n",
       "769   [(i,), (apologize for the delayed response,), ...   \n",
       "8339  [('nightglow i,), (i,), (imagine,), (be,), (st...   \n",
       "\n",
       "                                  Bigrams_with_chunking  \\\n",
       "8331  [('this, is), (is, i), (i, expected), (expecte...   \n",
       "1290  [(i, am), (am, busy), (busy, with work causes)...   \n",
       "1982  [('yes peace, is the absence of conflict), (is...   \n",
       "769   [(i, apologize for the delayed response), (apo...   \n",
       "8339  [('nightglow i, i), (i, imagine), (imagine, be...   \n",
       "\n",
       "                                 Trigrams_with_chunking  \n",
       "8331  [('this, is, i), (is, i, expected), (i, expect...  \n",
       "1290  [(i, am, busy), (am, busy, with work causes), ...  \n",
       "1982  [('yes peace, is the absence of conflict, the ...  \n",
       "769   [(i, apologize for the delayed response, for t...  \n",
       "8339  [('nightglow i, i, imagine), (i, imagine, be),...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_miniproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
