{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b2ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "from nltk.chunk import RegexpParser\n",
    "import contractions\n",
    "import re\n",
    "import emoji\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cd97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean raw text by removing URLs, emojis, mentions, and MBTI codes.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?\\S+|www\\S+', '', text)\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'@\\w+|#', '', text)\n",
    "    text = re.sub(r\"[^a-z\\']\", ' ', text)\n",
    "    # Remove MBTI type codes (e.g., INFJ, ENTP) to avoid leaking information\n",
    "    text = re.sub(r'\\b(I|E)(N|S)(F|T)(J|P)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Remove common footer\n",
    "    text = re.sub(r'\\bsent (from )?my \\w+(\\s\\w+)? using tapatalk\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'w w w', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_posts(posts_str: str) -> str:\n",
    "    \"\"\"Split multi-post string (separated by '|||'), clean each, and join.\"\"\"\n",
    "    posts = posts_str.split('|||')\n",
    "    cleaned_posts = [clean_text(post) for post in posts]\n",
    "    joined = ' '.join(cleaned_posts)\n",
    "    return re.sub(r'\\s+', ' ', joined).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d845da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding function\n",
    "def encode_mbti_type(mbti: str) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Encode MBTI type as 4 binary dimensions:\n",
    "    I/E â†’ 1/0, N/S â†’ 1/0, F/T â†’ 1/0, J/P â†’ 1/0\n",
    "    \"\"\"\n",
    "    return (\n",
    "        1 if mbti[0] == 'I' else 0,\n",
    "        1 if mbti[1] == 'N' else 0,\n",
    "        1 if mbti[2] == 'F' else 0,\n",
    "        1 if mbti[3] == 'J' else 0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e103189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mayasalkhateeb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize NLTK components\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "WNL = WordNetLemmatizer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    \"\"\"Map POS tag to WordNet format for lemmatization.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b28fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization variants\n",
    "def tokens_without_lemma(text: str) -> List[str]:\n",
    "    fixed = contractions.fix(text)\n",
    "    tokens = word_tokenize(fixed)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "    return filtered_tokens\n",
    "\n",
    "def tokens_with_lemma(text: str) -> List[str]:\n",
    "    fixed = contractions.fix(text)\n",
    "    tokens = word_tokenize(fixed)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "    lemmatized = [WNL.lemmatize(word.lower()) for word in filtered_tokens]\n",
    "    return lemmatized\n",
    "\n",
    "def tokens_with_lemma_pos(text: str) -> List[str]:\n",
    "    fixed = contractions.fix(text)\n",
    "    tokens = word_tokenize(fixed)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in STOP_WORDS]\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    lemmatized = [\n",
    "        WNL.lemmatize(token.lower(), pos=get_wordnet_pos(pos))\n",
    "        for token, pos in pos_tags\n",
    "    ]\n",
    "    return lemmatized\n",
    "\n",
    "def tokens_with_dependency_tree(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract simple dependency-like triples using heuristic rules (e.g., subj-verb-obj).\n",
    "    Returns list of strings like 'subj-verb-obj'.\n",
    "    \"\"\"\n",
    "    fixed = contractions.fix(text)\n",
    "    sentences = sent_tokenize(fixed)\n",
    "    relations = []\n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        if not tokens:\n",
    "            continue\n",
    "        pos = pos_tag(tokens)\n",
    "        for i in range(len(pos)):\n",
    "            if pos[i][1].startswith('VB'):\n",
    "                subj = None\n",
    "                obj = None\n",
    "                # Look left for subject (NN or PR)\n",
    "                for j in range(i - 1, -1, -1):\n",
    "                    if pos[j][1].startswith('NN') or pos[j][1].startswith('PR'):\n",
    "                        subj = pos[j][0]\n",
    "                        break\n",
    "                # Look right for object (NN or PR)\n",
    "                for j in range(i + 1, len(pos)):\n",
    "                    if pos[j][1].startswith('NN') or pos[j][1].startswith('PR'):\n",
    "                        obj = pos[j][0]\n",
    "                        break\n",
    "                if subj and obj:\n",
    "                    relation = f\"{subj}-{pos[i][0]}-{obj}\"\n",
    "                    relations.append(relation)\n",
    "    return relations\n",
    "\n",
    "def tokens_with_chunking(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract chunks (NP, PP, VP) using RegexpParser.\n",
    "    Returns list of chunk phrases as strings.\n",
    "    \"\"\"\n",
    "    fixed = contractions.fix(text)\n",
    "    sentences = sent_tokenize(fixed)\n",
    "    chunks = []\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT|JJ|NN.*>+}          # Noun phrases\n",
    "        PP: {<IN><NP>}               # Prepositional phrases\n",
    "        VP: {<VB.*><NP|PP>*}         # Verb phrases\n",
    "    \"\"\"\n",
    "    cp = RegexpParser(grammar)\n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        if not tokens:\n",
    "            continue\n",
    "        pos = pos_tag(tokens)\n",
    "        tree = cp.parse(pos)\n",
    "        for subtree in tree.subtrees(lambda t: t.label() in ['NP', 'PP', 'VP']):\n",
    "            chunk_text = ' '.join(word for word, tag in subtree.leaves())\n",
    "            chunks.append(chunk_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82380381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram generation\n",
    "def generate_ngrams(tokens: List[str]) -> Tuple[List[Tuple], List[Tuple], List[Tuple]]:\n",
    "    if not tokens:\n",
    "        return [], [], []\n",
    "    unigrams = list(ngrams(tokens, 1))\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    trigrams = list(ngrams(tokens, 3))\n",
    "    return unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef36da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:57<00:00, 150.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ·ï¸ Encoding MBTI types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:00<00:00, 1114583.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: without_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:25<00:00, 343.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for without_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:01<00:00, 4443.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_lemma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:43<00:00, 199.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_lemma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:01<00:00, 4779.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_lemma_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [04:30<00:00, 32.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_lemma_pos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:01<00:00, 4536.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_dep_tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [06:26<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_dep_tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:04<00:00, 1819.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Processing variant: with_chunking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [07:21<00:00, 19.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Generating n-grams for with_chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8675/8675 [00:02<00:00, 3941.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Main processing\n",
    "df = pd.read_csv(\"../data/raw/mbti_1.csv\")  # Adjust path if needed\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning posts...\")\n",
    "tqdm.pandas()\n",
    "df['cleaned_posts'] = df['posts'].progress_apply(preprocess_posts)\n",
    "\n",
    "print(\"ðŸ·ï¸ Encoding MBTI types...\")\n",
    "df[['IE', 'NS', 'FT', 'JP']] = pd.DataFrame(\n",
    "    df['type'].progress_apply(encode_mbti_type).tolist(),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "variants = [\n",
    "    'without_lemma',\n",
    "    'with_lemma',\n",
    "    'with_lemma_pos',\n",
    "    'with_dep_tree',\n",
    "    'with_chunking'\n",
    "]\n",
    "\n",
    "for var in variants:\n",
    "    print(f\"ðŸ”¤ Processing variant: {var}\")\n",
    "    if var == 'without_lemma':\n",
    "        func = tokens_without_lemma\n",
    "    elif var == 'with_lemma':\n",
    "        func = tokens_with_lemma\n",
    "    elif var == 'with_lemma_pos':\n",
    "        func = tokens_with_lemma_pos\n",
    "    elif var == 'with_dep_tree':\n",
    "        func = tokens_with_dependency_tree\n",
    "    elif var == 'with_chunking':\n",
    "        func = tokens_with_chunking\n",
    "    \n",
    "    df[f'tokens_{var}'] = df['cleaned_posts'].progress_apply(func)\n",
    "    \n",
    "    print(f\"ðŸ“Š Generating n-grams for {var}...\")\n",
    "    ngram_results = df[f'tokens_{var}'].progress_apply(generate_ngrams)\n",
    "    df[[f'Unigrams_{var}', f'Bigrams_{var}', f'Trigrams_{var}']] = pd.DataFrame(\n",
    "        ngram_results.tolist(), index=df.index\n",
    "    )\n",
    "\n",
    "# Drop intermediate columns\n",
    "df = df.drop(columns=['posts', 'cleaned_posts'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeaee952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Splitting data into train/test...\n",
      "âœ… Processing complete! Data saved to ../data/processed/train.pkl and test.pkl\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "print(\"âœ‚ï¸ Splitting data into train/test...\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['type'])\n",
    "\n",
    "# Save to ../data/processed\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "train_df.to_pickle('../data/processed/train.pkl')\n",
    "test_df.to_pickle('../data/processed/test.pkl')\n",
    "\n",
    "print(\"âœ… Processing complete! Data saved to ../data/processed/train.pkl and test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_miniproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
