{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "434d204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script loads the processed MBTI data and performs two experiments:\n",
    "1. Ontology-based features: Uses WordNet to extract hypernyms as semantic features, then trains Logistic Regression.\n",
    "2. Topic modeling: Uses LDA to extract topic distributions as features, then trains Logistic Regression.\n",
    "Evaluates both on the binary MBTI dimensions (IE, NS, FT, JP) using the test set.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import joblib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6b7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ensure NLTK WordNet is downloaded\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "def get_hypernyms(word: str, pos: str = 'n') -> List[str]:\n",
    "    \"\"\"\n",
    "    Get hypernyms for a word using WordNet.\n",
    "    Returns a list of hypernym lemmas at the first level.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    if not synsets:\n",
    "        return []\n",
    "    hypernyms = set()\n",
    "    for syn in synsets:\n",
    "        for hyper in syn.hypernyms():\n",
    "            hypernyms.update(lemma.name() for lemma in hyper.lemmas())\n",
    "    return list(hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4426462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ontology_features(texts: pd.Series, max_features: int = 1000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract ontology-based features using WordNet hypernyms.\n",
    "    - Tokenize and get hypernyms for nouns/verbs.\n",
    "    - Use CountVectorizer on the hypernyms as 'semantic bag-of-words'.\n",
    "    \"\"\"\n",
    "    hypernym_docs = []\n",
    "    for text in tqdm(texts, desc=\"Extracting hypernyms\"):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "        hypernyms = []\n",
    "        for word, tag in pos_tags:\n",
    "            if tag.startswith('N') or tag.startswith('V'):\n",
    "                hypers = get_hypernyms(word.lower(), pos='n' if tag.startswith('N') else 'v')\n",
    "                hypernyms.extend(hypers)\n",
    "        hypernym_docs.append(' '.join(set(hypernyms)))  # Unique hypernyms per doc\n",
    "    \n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    features = vectorizer.fit_transform(hypernym_docs).toarray()\n",
    "    logger.info(f\"Ontology features shape: {features.shape}\")\n",
    "    return features, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30d7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling_features(texts: pd.Series, n_topics: int = 20, max_features: int = 5000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract topic distributions using LDA.\n",
    "    - Vectorize with TF-IDF.\n",
    "    - Fit LDA to get topic probabilities per document.\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(max_features=max_features)\n",
    "    X_tfidf = tfidf.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    features = lda.fit_transform(X_tfidf)\n",
    "    logger.info(f\"LDA features shape: {features.shape}\")\n",
    "    return features, (tfidf, lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27c89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_binary(X_train: np.ndarray, X_test: np.ndarray, train_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "                          feature_type: str, use_smote: bool = True):\n",
    "    \"\"\"\n",
    "    Train and evaluate Logistic Regression on binary MBTI dimensions.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        y_train = train_df[dim]\n",
    "        y_test = test_df[dim]\n",
    "        \n",
    "        if use_smote:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            X_train_res, y_train_res = X_train, y_train\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=1000)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "        \n",
    "        # Save model (optional)\n",
    "        model_path = f'../models/binary_{dim}_{feature_type}.pkl'\n",
    "        joblib.dump(clf, model_path)\n",
    "        \n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        results[dim] = {'accuracy': acc, 'f1': f1}\n",
    "        \n",
    "        logger.info(f\"{feature_type} - {dim} - Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "        logger.info(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4375702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 14:44:31,662 - INFO - Loaded train (6940) and test (1735) data.\n",
      "2025-11-22 14:44:31,663 - INFO - Generating ontology features...\n",
      "Extracting hypernyms: 100%|██████████| 6940/6940 [09:40<00:00, 11.95it/s]\n",
      "2025-11-22 14:54:24,450 - INFO - Ontology features shape: (6940, 1000)\n",
      "Test hypernyms: 100%|██████████| 1735/1735 [02:20<00:00, 12.38it/s]\n",
      "2025-11-22 14:56:55,167 - INFO - ontology - IE - Accuracy: 0.6427, F1: 0.6497\n",
      "2025-11-22 14:56:55,180 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.29      0.27       401\n",
      "           1       0.78      0.75      0.76      1334\n",
      "\n",
      "    accuracy                           0.64      1735\n",
      "   macro avg       0.52      0.52      0.52      1735\n",
      "weighted avg       0.66      0.64      0.65      1735\n",
      "\n",
      "2025-11-22 14:57:02,857 - INFO - ontology - NS - Accuracy: 0.7389, F1: 0.7587\n",
      "2025-11-22 14:57:02,871 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.29      0.24       240\n",
      "           1       0.88      0.81      0.84      1495\n",
      "\n",
      "    accuracy                           0.74      1735\n",
      "   macro avg       0.54      0.55      0.54      1735\n",
      "weighted avg       0.78      0.74      0.76      1735\n",
      "\n",
      "2025-11-22 14:57:08,797 - INFO - ontology - FT - Accuracy: 0.6196, F1: 0.6191\n",
      "2025-11-22 14:57:08,814 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58       796\n",
      "           1       0.65      0.66      0.65       939\n",
      "\n",
      "    accuracy                           0.62      1735\n",
      "   macro avg       0.62      0.62      0.62      1735\n",
      "weighted avg       0.62      0.62      0.62      1735\n",
      "\n",
      "2025-11-22 14:57:16,238 - INFO - ontology - JP - Accuracy: 0.5331, F1: 0.5325\n",
      "2025-11-22 14:57:16,252 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.62      0.62      1048\n",
      "           1       0.41      0.40      0.41       687\n",
      "\n",
      "    accuracy                           0.53      1735\n",
      "   macro avg       0.51      0.51      0.51      1735\n",
      "weighted avg       0.53      0.53      0.53      1735\n",
      "\n",
      "2025-11-22 14:57:16,265 - INFO - Generating topic modeling features...\n",
      "2025-11-22 14:57:54,670 - INFO - LDA features shape: (6940, 20)\n",
      "2025-11-22 14:57:56,732 - INFO - topics - IE - Accuracy: 0.2311, F1: 0.0868\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025-11-22 14:57:56,752 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      1.00      0.38       401\n",
      "           1       0.00      0.00      0.00      1334\n",
      "\n",
      "    accuracy                           0.23      1735\n",
      "   macro avg       0.12      0.50      0.19      1735\n",
      "weighted avg       0.05      0.23      0.09      1735\n",
      "\n",
      "2025-11-22 14:57:56,794 - INFO - topics - NS - Accuracy: 0.7072, F1: 0.7384\n",
      "2025-11-22 14:57:56,810 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.33      0.24       240\n",
      "           1       0.88      0.77      0.82      1495\n",
      "\n",
      "    accuracy                           0.71      1735\n",
      "   macro avg       0.53      0.55      0.53      1735\n",
      "weighted avg       0.78      0.71      0.74      1735\n",
      "\n",
      "2025-11-22 14:57:56,861 - INFO - topics - FT - Accuracy: 0.4588, F1: 0.2886\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025-11-22 14:57:56,877 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      1.00      0.63       796\n",
      "           1       0.00      0.00      0.00       939\n",
      "\n",
      "    accuracy                           0.46      1735\n",
      "   macro avg       0.23      0.50      0.31      1735\n",
      "weighted avg       0.21      0.46      0.29      1735\n",
      "\n",
      "2025-11-22 14:57:56,917 - INFO - topics - JP - Accuracy: 0.6040, F1: 0.4549\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mayasalkhateeb/Desktop/HIAST/NLP/Lab/mbti-personality-classifier/nlp_miniproj/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "2025-11-22 14:57:56,934 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75      1048\n",
      "           1       0.00      0.00      0.00       687\n",
      "\n",
      "    accuracy                           0.60      1735\n",
      "   macro avg       0.30      0.50      0.38      1735\n",
      "weighted avg       0.36      0.60      0.45      1735\n",
      "\n",
      "2025-11-22 14:57:56,935 - INFO - Ontology Results:\n",
      "2025-11-22 14:57:56,936 - INFO - IE: Acc=0.6427, F1=0.6497\n",
      "2025-11-22 14:57:56,937 - INFO - NS: Acc=0.7389, F1=0.7587\n",
      "2025-11-22 14:57:56,937 - INFO - FT: Acc=0.6196, F1=0.6191\n",
      "2025-11-22 14:57:56,939 - INFO - JP: Acc=0.5331, F1=0.5325\n",
      "2025-11-22 14:57:56,939 - INFO - Topic Modeling Results:\n",
      "2025-11-22 14:57:56,940 - INFO - IE: Acc=0.2311, F1=0.0868\n",
      "2025-11-22 14:57:56,941 - INFO - NS: Acc=0.7072, F1=0.7384\n",
      "2025-11-22 14:57:56,942 - INFO - FT: Acc=0.4588, F1=0.2886\n",
      "2025-11-22 14:57:56,943 - INFO - JP: Acc=0.6040, F1=0.4549\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load processed data\n",
    "    train_df = pd.read_pickle('../data/processed/train.pkl')\n",
    "    test_df = pd.read_pickle('../data/processed/test.pkl')\n",
    "    logger.info(f\"Loaded train ({len(train_df)}) and test ({len(test_df)}) data.\")\n",
    "    \n",
    "    # Use 'cleaned_text' column\n",
    "    X_train_text = train_df['cleaned_posts']\n",
    "    X_test_text = test_df['cleaned_posts']\n",
    "    \n",
    "    # 1. Ontology-based model\n",
    "    logger.info(\"Generating ontology features...\")\n",
    "    X_train_ontology, ontology_vectorizer = ontology_features(X_train_text)\n",
    "    X_test_hypernym_docs = []\n",
    "    \n",
    "    for text in tqdm(X_test_text, desc=\"Test hypernyms\"):\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        hypernyms = []\n",
    "        for word, tag in pos_tags:\n",
    "            if tag.startswith('N') or tag.startswith('V'):\n",
    "                pos_wn = 'n' if tag.startswith('N') else 'v'\n",
    "                hypers = get_hypernyms(word.lower(), pos=pos_wn)\n",
    "                hypernyms.extend(hypers)\n",
    "        X_test_hypernym_docs.append(' '.join(set(hypernyms)))\n",
    "    \n",
    "    X_test_ontology = ontology_vectorizer.transform(X_test_hypernym_docs).toarray()\n",
    "    \n",
    "    ontology_results = train_evaluate_binary(X_train_ontology, X_test_ontology, train_df, test_df, \"ontology\")\n",
    "    \n",
    "    # 2. Topic modeling-based model\n",
    "    logger.info(\"Generating topic modeling features...\")\n",
    "    X_train_topics, (topic_tfidf, topic_lda) = topic_modeling_features(X_train_text)\n",
    "    X_test_tfidf = topic_tfidf.transform(X_test_text)\n",
    "    X_test_topics = topic_lda.transform(X_test_tfidf)\n",
    "    \n",
    "    topic_results = train_evaluate_binary(X_train_topics, X_test_topics, train_df, test_df, \"topics\")\n",
    "    \n",
    "    # Compare results\n",
    "    logger.info(\"Ontology Results:\")\n",
    "    for dim, res in ontology_results.items():\n",
    "        logger.info(f\"{dim}: Acc={res['accuracy']:.4f}, F1={res['f1']:.4f}\")\n",
    "    \n",
    "    logger.info(\"Topic Modeling Results:\")\n",
    "    for dim, res in topic_results.items():\n",
    "        logger.info(f\"{dim}: Acc={res['accuracy']:.4f}, F1={res['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_miniproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
