{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6691dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7692cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading processed data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"üìÇ Loading processed data...\")\n",
    "train_df = pd.read_pickle('../data/processed/train.pkl')\n",
    "test_df = pd.read_pickle('../data/processed/test.pkl')\n",
    "\n",
    "# Variants\n",
    "variants = [\n",
    "    'without_lemma',\n",
    "    'with_lemma',\n",
    "    'with_lemma_pos',\n",
    "    'with_dep_tree',\n",
    "    'with_chunking'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa935ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare text from tokens/ngrams\n",
    "def prepare_text(tokens: List, ngrams_b: List[Tuple], ngrams_t: List[Tuple], use_ngrams: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Convert tokens and optionally ngrams to string for TF-IDF.\n",
    "    For ngrams, join tuples into space-separated strings.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return ''\n",
    "    # Handle different token formats (str or tuples)\n",
    "    if isinstance(tokens[0], str):\n",
    "        text = ' '.join(tokens)\n",
    "    elif isinstance(tokens[0], tuple):\n",
    "        text = ' '.join(['_'.join(t) for t in tokens])\n",
    "    else:\n",
    "        text = ''\n",
    "    \n",
    "    if use_ngrams:\n",
    "        bigrams_str = ' '.join(['_'.join(gram) for gram in ngrams_b]) if ngrams_b else ''\n",
    "        trigrams_str = ' '.join(['_'.join(gram) for gram in ngrams_t]) if ngrams_t else ''\n",
    "        text = f\"{text} {bigrams_str} {trigrams_str}\".strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7213cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results storage\n",
    "results_multi = {}\n",
    "results_binary = {dim: {} for dim in ['IE', 'NS', 'FT', 'JP']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing variant: without_lemma\n",
      "üìù Preparing text features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6940/6940 [00:03<00:00, 2006.97it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1735/1735 [00:00<00:00, 2529.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Training binary model for IE with SMOTE...\n"
     ]
    }
   ],
   "source": [
    "# Loop over variants\n",
    "for var in variants:\n",
    "    print(f\"\\nüîç Processing variant: {var}\")\n",
    "    \n",
    "    # Prepare train and test texts\n",
    "    print(\"üìù Preparing text features...\")\n",
    "    tqdm.pandas()\n",
    "    train_df['text'] = train_df.progress_apply(\n",
    "        lambda row: prepare_text(row[f'tokens_{var}'], row[f'Bigrams_{var}'], row[f'Trigrams_{var}'], use_ngrams=True), axis=1\n",
    "    )\n",
    "    test_df['text'] = test_df.progress_apply(\n",
    "        lambda row: prepare_text(row[f'tokens_{var}'], row[f'Bigrams_{var}'], row[f'Trigrams_{var}'], use_ngrams=True), axis=1\n",
    "    )\n",
    "    \n",
    "    X_train = train_df['text']\n",
    "    X_test = test_df['text']\n",
    "    \n",
    "    # Multi-class (16 types)\n",
    "    # print(\"üß† Training multi-class model with balanced class weights...\")\n",
    "    # y_train_multi = train_df['type']\n",
    "    # y_test_multi = test_df['type']\n",
    "    \n",
    "    # pipeline_multi = Pipeline([\n",
    "    #     ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,3))),\n",
    "    #     ('clf', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, class_weight='balanced'))\n",
    "    # ])\n",
    "    # params = {'clf__C': [0.1, 1, 10], 'clf__solver': ['lbfgs', 'liblinear']}\n",
    "    # grid = GridSearchCV(pipeline_multi, params, cv=5, scoring='f1_weighted')\n",
    "    # grid.fit(X_train, y_train_multi)\n",
    "    \n",
    "    # pipeline_multi.fit(X_train, y_train_multi)\n",
    "    \n",
    "    # y_pred_multi = pipeline_multi.predict(X_test)\n",
    "    \n",
    "    # acc_multi = accuracy_score(y_test_multi, y_pred_multi)\n",
    "    # f1_multi = f1_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "    \n",
    "    # results_multi[var] = {'accuracy': acc_multi, 'f1': f1_multi}\n",
    "    # print(f\"Multi-class - Accuracy: {acc_multi:.4f}, F1: {f1_multi:.4f}\")\n",
    "    # print(classification_report(y_test_multi, y_pred_multi))\n",
    "    \n",
    "    # Save model\n",
    "    # os.makedirs('models', exist_ok=True)\n",
    "    # joblib.dump(pipeline_multi, f'models/multi_{var}.pkl')\n",
    "    \n",
    "    # Binary classifiers for each dimension\n",
    "    for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "        print(f\"üß† Training binary model for {dim} with SMOTE...\")\n",
    "        y_train_bin = train_df[dim]\n",
    "        y_test_bin = test_df[dim]\n",
    "        \n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n",
    "        X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "        X_test_tfidf = tfidf.transform(X_test).toarray()  \n",
    "        \n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train_bin)\n",
    "        \n",
    "        clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "        clf.fit(X_train_res, y_train_res)\n",
    "        y_pred_bin = clf.predict(X_test_tfidf)\n",
    "        \n",
    "        acc_bin = accuracy_score(y_test_bin, y_pred_bin)\n",
    "        f1_bin = f1_score(y_test_bin, y_pred_bin, average='weighted')\n",
    "        \n",
    "        results_binary[dim][var] = {'accuracy': acc_bin, 'f1': f1_bin}\n",
    "        print(f\"{dim} - Accuracy: {acc_bin:.4f}, F1: {f1_bin:.4f}\")\n",
    "        print(classification_report(y_test_bin, y_pred_bin))\n",
    "    \n",
    "        joblib.dump((tfidf, clf), f'../models/binary_{dim}_{var}.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a24aa196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Comparison of Multi-class Results:\n",
      "without_lemma: Accuracy=0.3527, F1=0.3671\n",
      "with_lemma: Accuracy=0.3470, F1=0.3608\n",
      "with_lemma_pos: Accuracy=0.3412, F1=0.3572\n",
      "with_dep_tree: Accuracy=0.1516, F1=0.1692\n",
      "with_chunking: Accuracy=0.2767, F1=0.2926\n",
      "\n",
      "üìä Comparison of Binary IE Results:\n",
      "without_lemma: Accuracy=0.7308, F1=0.7447\n",
      "with_lemma: Accuracy=0.7297, F1=0.7436\n",
      "with_lemma_pos: Accuracy=0.7222, F1=0.7368\n",
      "with_dep_tree: Accuracy=0.6571, F1=0.6775\n",
      "with_chunking: Accuracy=0.7037, F1=0.7196\n",
      "\n",
      "üìä Comparison of Binary NS Results:\n",
      "without_lemma: Accuracy=0.7856, F1=0.8067\n",
      "with_lemma: Accuracy=0.7844, F1=0.8053\n",
      "with_lemma_pos: Accuracy=0.7850, F1=0.8061\n",
      "with_dep_tree: Accuracy=0.6928, F1=0.7291\n",
      "with_chunking: Accuracy=0.7556, F1=0.7813\n",
      "\n",
      "üìä Comparison of Binary FT Results:\n",
      "without_lemma: Accuracy=0.7821, F1=0.7824\n",
      "with_lemma: Accuracy=0.7879, F1=0.7882\n",
      "with_lemma_pos: Accuracy=0.7775, F1=0.7779\n",
      "with_dep_tree: Accuracy=0.6496, F1=0.6501\n",
      "with_chunking: Accuracy=0.7487, F1=0.7490\n",
      "\n",
      "üìä Comparison of Binary JP Results:\n",
      "without_lemma: Accuracy=0.6663, F1=0.6689\n",
      "with_lemma: Accuracy=0.6692, F1=0.6719\n",
      "with_lemma_pos: Accuracy=0.6622, F1=0.6649\n",
      "with_dep_tree: Accuracy=0.5775, F1=0.5816\n",
      "with_chunking: Accuracy=0.6254, F1=0.6284\n",
      "‚úÖ Training and evaluation complete! Models saved in models/ directory.\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"\\nüìä Comparison of Multi-class Results:\")\n",
    "for var, res in results_multi.items():\n",
    "    print(f\"{var}: Accuracy={res['accuracy']:.4f}, F1={res['f1']:.4f}\")\n",
    "\n",
    "for dim in ['IE', 'NS', 'FT', 'JP']:\n",
    "    print(f\"\\nüìä Comparison of Binary {dim} Results:\")\n",
    "    for var, res in results_binary[dim].items():\n",
    "        print(f\"{var}: Accuracy={res['accuracy']:.4f}, F1={res['f1']:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training and evaluation complete! Models saved in models/ directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_miniproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
