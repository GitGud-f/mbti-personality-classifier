{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKLqFXh4jGcI",
        "outputId": "475961be-fb7f-4f52-bb1a-12e9fb45d49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies & Import Libraries\n",
        "# !pip install transformers torch scikit-learn pandas numpy\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh5WsCWmjWQ7",
        "outputId": "603a44f8-b554-4f39-b381-c39767baed76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset loaded. Shape: (8675, 2)\n",
            "   type  IE  NS  TF  JP\n",
            "0  INFJ   0   0   1   0\n",
            "1  ENTP   1   0   0   1\n",
            "2  INTP   0   0   0   1\n",
            "3  INTJ   0   0   0   0\n",
            "4  ENTJ   1   0   0   0\n"
          ]
        }
      ],
      "source": [
        "# @title 2. Load Data & Preprocess Labels\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# UPDATE THIS PATH to where your dataset is stored on Drive\n",
        "FILE_PATH = '/content/drive/MyDrive/mbti_1.csv'\n",
        "# ---------------------\n",
        "\n",
        "def load_and_encode_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"Dataset loaded. Shape: {df.shape}\")\n",
        "\n",
        "    # Create 4 binary columns\n",
        "    # 0 for first letter (I, N, F, P), 1 for second letter (E, S, T, J)\n",
        "    # Note: You can adjust mapping preference.\n",
        "    # Standard: I-E, N-S, F-T, P-J\n",
        "\n",
        "    df['IE'] = df['type'].apply(lambda x: 0 if 'I' in x else 1) # 0=Introvert, 1=Extrovert\n",
        "    df['NS'] = df['type'].apply(lambda x: 0 if 'N' in x else 1) # 0=Intuition, 1=Sensing\n",
        "    df['TF'] = df['type'].apply(lambda x: 0 if 'T' in x else 1) # 0=Thinking, 1=Feeling\n",
        "    df['JP'] = df['type'].apply(lambda x: 0 if 'J' in x else 1) # 0=Judging, 1=Perceiving\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    df = load_and_encode_data(FILE_PATH)\n",
        "    print(df[['type', 'IE', 'NS', 'TF', 'JP']].head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File not found. Please check FILE_PATH.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_mDEgpnjZZO",
        "outputId": "336a7335-044c-4d4c-f3fb-a4608d696748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning texts... (this might take a minute)\n"
          ]
        }
      ],
      "source": [
        "# @title 3. Text Cleaning & Custom Dataset Class\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    1. Remove URLs\n",
        "    2. Remove pipe separators |||\n",
        "    3. Lowercase\n",
        "    \"\"\"\n",
        "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
        "    text = text.replace('|||', ' ')     # Replace separators with space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove multiple spaces\n",
        "    return text.lower()\n",
        "\n",
        "print(\"Cleaning texts... (this might take a minute)\")\n",
        "df['cleaned_posts'] = df['posts'].apply(clean_text)\n",
        "\n",
        "class MBTIDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vByPT4L2jb-b"
      },
      "outputs": [],
      "source": [
        "# @title 4. Generic Training & Evaluation Loop\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=targets\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=targets\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions.double() / n_examples\n",
        "    return accuracy, np.mean(losses), all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aiLLZHOvUE_",
        "outputId": "aba27918-a5ad-4bcd-9c71-231cd35935e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            " TRAINING CLASSIFIER FOR: IE (With Class Weights)\n",
            "==============================\n",
            "Class Weights for IE: [0.64969107 2.17010632]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train loss 0.6307 accuracy 0.6912\n",
            "Val   loss 0.6148 accuracy 0.7366\n",
            "Epoch 2/5\n",
            "Train loss 0.5369 accuracy 0.7677\n",
            "Val   loss 0.5298 accuracy 0.7550\n",
            "Epoch 3/5\n",
            "Train loss 0.4255 accuracy 0.8346\n",
            "Val   loss 0.5014 accuracy 0.7862\n",
            "Epoch 4/5\n",
            "Train loss 0.2859 accuracy 0.9102\n",
            "Val   loss 0.5339 accuracy 0.7988\n",
            "Epoch 5/5\n",
            "Train loss 0.1928 accuracy 0.9471\n",
            "Val   loss 0.6091 accuracy 0.8023\n",
            "\n",
            "--- Report for IE ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.86      0.89      0.87      1335\n",
            "     Class 1       0.58      0.53      0.55       400\n",
            "\n",
            "    accuracy                           0.80      1735\n",
            "   macro avg       0.72      0.71      0.71      1735\n",
            "weighted avg       0.80      0.80      0.80      1735\n",
            "\n",
            "\n",
            "==============================\n",
            " TRAINING CLASSIFIER FOR: NS (With Class Weights)\n",
            "==============================\n",
            "Class Weights for NS: [0.58007355 3.62212944]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train loss 0.6607 accuracy 0.7138\n",
            "Val   loss 0.4891 accuracy 0.8231\n",
            "Epoch 2/5\n",
            "Train loss 0.5449 accuracy 0.8133\n",
            "Val   loss 0.5990 accuracy 0.7424\n",
            "Epoch 3/5\n",
            "Train loss 0.4427 accuracy 0.8736\n",
            "Val   loss 0.4516 accuracy 0.8144\n",
            "Epoch 4/5\n",
            "Train loss 0.3580 accuracy 0.9190\n",
            "Val   loss 0.5329 accuracy 0.8259\n",
            "Epoch 5/5\n",
            "Train loss 0.2771 accuracy 0.9510\n",
            "Val   loss 0.5795 accuracy 0.8277\n",
            "\n",
            "--- Report for NS ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.91      0.89      0.90      1496\n",
            "     Class 1       0.38      0.42      0.40       239\n",
            "\n",
            "    accuracy                           0.83      1735\n",
            "   macro avg       0.65      0.66      0.65      1735\n",
            "weighted avg       0.83      0.83      0.83      1735\n",
            "\n",
            "\n",
            "==============================\n",
            " TRAINING CLASSIFIER FOR: TF (With Class Weights)\n",
            "==============================\n",
            "Class Weights for TF: [1.08948195 0.9241012 ]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train loss 0.5990 accuracy 0.6764\n",
            "Val   loss 0.4996 accuracy 0.7602\n",
            "Epoch 2/5\n",
            "Train loss 0.4465 accuracy 0.8030\n",
            "Val   loss 0.4961 accuracy 0.7712\n",
            "Epoch 3/5\n",
            "Train loss 0.3078 accuracy 0.8805\n",
            "Val   loss 0.5674 accuracy 0.7671\n",
            "Epoch 4/5\n",
            "Train loss 0.1881 accuracy 0.9393\n",
            "Val   loss 0.7530 accuracy 0.7689\n",
            "Epoch 5/5\n",
            "Train loss 0.1171 accuracy 0.9679\n",
            "Val   loss 0.9416 accuracy 0.7735\n",
            "\n",
            "--- Report for TF ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.78      0.71      0.74       796\n",
            "     Class 1       0.77      0.83      0.80       939\n",
            "\n",
            "    accuracy                           0.77      1735\n",
            "   macro avg       0.77      0.77      0.77      1735\n",
            "weighted avg       0.77      0.77      0.77      1735\n",
            "\n",
            "\n",
            "==============================\n",
            " TRAINING CLASSIFIER FOR: JP (With Class Weights)\n",
            "==============================\n",
            "Class Weights for JP: [1.26319621 0.82756976]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "Train loss 0.6367 accuracy 0.6236\n",
            "Val   loss 0.5700 accuracy 0.7170\n",
            "Epoch 2/5\n",
            "Train loss 0.5044 accuracy 0.7501\n",
            "Val   loss 0.5653 accuracy 0.7003\n",
            "Epoch 3/5\n",
            "Train loss 0.3661 accuracy 0.8452\n",
            "Val   loss 0.6698 accuracy 0.6951\n",
            "Epoch 4/5\n",
            "Train loss 0.2203 accuracy 0.9216\n",
            "Val   loss 0.9827 accuracy 0.6818\n",
            "Epoch 5/5\n",
            "Train loss 0.1348 accuracy 0.9581\n",
            "Val   loss 1.0566 accuracy 0.7020\n",
            "\n",
            "--- Report for JP ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.63      0.61      0.62       687\n",
            "     Class 1       0.75      0.76      0.76      1048\n",
            "\n",
            "    accuracy                           0.70      1735\n",
            "   macro avg       0.69      0.69      0.69      1735\n",
            "weighted avg       0.70      0.70      0.70      1735\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title 4 & 5 Updated: Training with Class Weights (Replaces previous logic)\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Modified Training Function to accept Weights ---\n",
        "def train_epoch_weighted(model, data_loader, optimizer, device, scheduler, n_examples, class_weights):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Define Loss function with weights\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"labels\"].to(device)\n",
        "\n",
        "        # Note: We do NOT use model(labels=targets) here because we need custom loss\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Calculate loss manually using our weighted function\n",
        "        loss = loss_fn(logits, targets)\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "# --- Main Execution Loop ---\n",
        "\n",
        "BERT_MODEL_NAME = 'bert-base-uncased'\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5 # Increased slightly as weighted loss stabilizes slowly\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "dimensions = ['IE', 'NS', 'TF', 'JP']\n",
        "results_store = {}\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "for dim in dimensions:\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\" TRAINING CLASSIFIER FOR: {dim} (With Class Weights)\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    X = df['cleaned_posts'].values\n",
        "    y = df[dim].values\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
        "\n",
        "    # --- CALCULATE CLASS WEIGHTS HERE ---\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train\n",
        "    )\n",
        "    # Convert to Tensor and push to GPU\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    print(f\"Class Weights for {dim}: {class_weights}\")\n",
        "    # ------------------------------------\n",
        "\n",
        "    train_dataset = MBTIDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "    test_dataset = MBTIDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Pass weights_tensor to the training function\n",
        "        train_acc, train_loss = train_epoch_weighted(\n",
        "            model, train_data_loader, optimizer, device, scheduler, len(train_dataset), weights_tensor\n",
        "        )\n",
        "        print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n",
        "\n",
        "        # Use previous eval function\n",
        "        val_acc, val_loss, preds, labels = eval_model(\n",
        "            model, test_data_loader, device, len(test_dataset)\n",
        "        )\n",
        "        print(f'Val   loss {val_loss:.4f} accuracy {val_acc:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "    print(f\"\\n--- Report for {dim} ---\")\n",
        "    print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))\n",
        "    results_store[dim] = val_acc.item()\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aU6VmD7vXea"
      },
      "source": [
        "-------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
