{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKLqFXh4jGcI",
        "outputId": "924e3a7f-4b94-43ec-b1bb-b53553148846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies & Import Libraries\n",
        "# !pip install transformers torch scikit-learn pandas numpy\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Load Data & Preprocess Labels\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# UPDATE THIS PATH to where your dataset is stored on Drive\n",
        "FILE_PATH = '/content/drive/MyDrive/mbti_1.csv'\n",
        "# ---------------------\n",
        "\n",
        "def load_and_encode_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"Dataset loaded. Shape: {df.shape}\")\n",
        "\n",
        "    # Create 4 binary columns\n",
        "    # 0 for first letter (I, N, F, P), 1 for second letter (E, S, T, J)\n",
        "    # Note: You can adjust mapping preference.\n",
        "    # Standard: I-E, N-S, F-T, P-J\n",
        "\n",
        "    df['IE'] = df['type'].apply(lambda x: 0 if 'I' in x else 1) # 0=Introvert, 1=Extrovert\n",
        "    df['NS'] = df['type'].apply(lambda x: 0 if 'N' in x else 1) # 0=Intuition, 1=Sensing\n",
        "    df['TF'] = df['type'].apply(lambda x: 0 if 'T' in x else 1) # 0=Thinking, 1=Feeling\n",
        "    df['JP'] = df['type'].apply(lambda x: 0 if 'J' in x else 1) # 0=Judging, 1=Perceiving\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "try:\n",
        "    df = load_and_encode_data(FILE_PATH)\n",
        "    print(df[['type', 'IE', 'NS', 'TF', 'JP']].head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File not found. Please check FILE_PATH.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh5WsCWmjWQ7",
        "outputId": "16c8acc9-1de6-461e-943d-2991d471109b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset loaded. Shape: (8675, 2)\n",
            "   type  IE  NS  TF  JP\n",
            "0  INFJ   0   0   1   0\n",
            "1  ENTP   1   0   0   1\n",
            "2  INTP   0   0   0   1\n",
            "3  INTJ   0   0   0   0\n",
            "4  ENTJ   1   0   0   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Text Cleaning & Custom Dataset Class\n",
        "import string\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'https?\\S+|www\\S+', '', text)\n",
        "  text = re.sub(r'@\\w+|#', '', text)\n",
        "\n",
        "  mbti_types = [\n",
        "            \"infj\",\n",
        "            \"entp\",\n",
        "            \"intp\",\n",
        "            \"intj\",\n",
        "            \"entj\",\n",
        "            \"enfj\",\n",
        "            \"infp\",\n",
        "            \"enfp\",\n",
        "            \"isfp\",\n",
        "            \"istp\",\n",
        "            \"isfj\",\n",
        "            \"istj\",\n",
        "            \"estp\",\n",
        "            \"esfp\",\n",
        "            \"estj\",\n",
        "          \"esfj\",\n",
        "  ]\n",
        "  pattern = r\"\\b(\" + \"|\".join(mbti_types) + r\")\\b\"\n",
        "  text = re.sub(pattern, \"\", text)\n",
        "  text = re.sub(r'\\bsent (from )?my \\w+(\\s\\w+)? using tapatalk\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "\n",
        "  text = text.replace(\"|||\", \" \")\n",
        "\n",
        "  text = re.sub(r'w w w', '', text)\n",
        "\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "print(\"Cleaning texts... (this might take a minute)\")\n",
        "df['cleaned_posts'] = df['posts'].apply(clean_text)\n",
        "\n",
        "class MBTIDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_mDEgpnjZZO",
        "outputId": "f39eea6a-eb13-482e-97a8-a5ba53716d07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning texts... (this might take a minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Generic Training & Evaluation Loop\n",
        "\n",
        "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=targets\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=targets\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions.double() / n_examples\n",
        "    return accuracy, np.mean(losses), all_preds, all_labels"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vByPT4L2jb-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=targets\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(targets.cpu().numpy())\n",
        "\n",
        "    accuracy = correct_predictions.double() / n_examples\n",
        "    return accuracy, np.mean(losses), all_preds, all_labels"
      ],
      "metadata": {
        "id": "SjmT667vy-MW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- HELPER: Layer Freezing Logic ---\n",
        "def freeze_roberta_layers(model, unfreeze_last_n_layers=2):\n",
        "    \"\"\"\n",
        "    Freezes all layers except the classifier and the last N encoder layers.\n",
        "    \"\"\"\n",
        "    # 1. Freeze everything first\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 2. Unfreeze the Classifier Head (The part we initialized randomly)\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # 3. Unfreeze the last N layers of the Encoder\n",
        "    # RoBERTa base has 12 layers (0-11).\n",
        "    # If unfreeze_last_n_layers = 2, we want to train layer 10 and 11.\n",
        "    total_layers = model.config.num_hidden_layers\n",
        "\n",
        "    for i in range(total_layers - unfreeze_last_n_layers, total_layers):\n",
        "        for param in model.roberta.encoder.layer[i].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Verify optimization\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    all_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\" -> Layer Freezing Applied. Trainable Params: {trainable_params:,} / {all_params:,} ({(trainable_params/all_params):.1%})\")\n",
        "\n",
        "# --- Training Function (Weighted) ---\n",
        "def train_epoch_weighted(model, data_loader, optimizer, device, scheduler, n_examples, class_weights):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    for d in data_loader:\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, targets)\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "ROBERTA_MODEL_NAME = 'roberta-base' # Changed from BERT\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-5\n",
        "UNFREEZE_LAST_N = 2 # We will only train the last 2 layers + Classifier\n",
        "\n",
        "dimensions = ['IE', 'NS', 'TF', 'JP']\n",
        "results_store = {}\n",
        "\n",
        "# Use RoBERTa Tokenizer\n",
        "print(f\"Loading Tokenizer: {ROBERTA_MODEL_NAME}\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n",
        "\n",
        "for dim in dimensions:\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\" TRAINING RoBERTa (Frozen) FOR: {dim}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    X = df['cleaned_posts'].values\n",
        "    y = df[dim].values\n",
        "\n",
        "    # Stratified Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
        "\n",
        "    # Compute Class Weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset = MBTIDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
        "    test_dataset = MBTIDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize RoBERTa\n",
        "    model = RobertaForSequenceClassification.from_pretrained(ROBERTA_MODEL_NAME, num_labels=2)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # --- APPLY FREEZING ---\n",
        "    freeze_roberta_layers(model, unfreeze_last_n_layers=UNFREEZE_LAST_N)\n",
        "    # ----------------------\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    total_steps = len(train_data_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        train_acc, train_loss = train_epoch_weighted(\n",
        "            model, train_data_loader, optimizer, device, scheduler, len(train_dataset), weights_tensor\n",
        "        )\n",
        "        print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n",
        "\n",
        "        val_acc, val_loss, preds, labels = eval_model(\n",
        "            model, test_data_loader, device, len(test_dataset)\n",
        "        )\n",
        "        print(f'Val   loss {val_loss:.4f} accuracy {val_acc:.4f}')\n",
        "\n",
        "    print(f\"\\n--- Report for {dim} ---\")\n",
        "    print(classification_report(labels, preds, target_names=['Class 0', 'Class 1']))\n",
        "    results_store[dim] = val_acc.item()\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aiLLZHOvUE_",
        "outputId": "4b30d24c-b02c-4a30-d3cc-bbb60f91428c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Tokenizer: roberta-base\n",
            "\n",
            "========================================\n",
            " TRAINING RoBERTa (Frozen) FOR: IE\n",
            "========================================\n",
            "Class Weights: [0.64969107 2.17010632]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Layer Freezing Applied. Trainable Params: 14,767,874 / 124,647,170 (11.8%)\n",
            "Epoch 1/5\n",
            "Train loss 0.6875 accuracy 0.5529\n",
            "Val   loss 0.5525 accuracy 0.7429\n",
            "Epoch 2/5\n",
            "Train loss 0.6477 accuracy 0.6370\n",
            "Val   loss 0.5154 accuracy 0.7493\n",
            "Epoch 3/5\n",
            "Train loss 0.6203 accuracy 0.6764\n",
            "Val   loss 0.5230 accuracy 0.7493\n",
            "Epoch 4/5\n",
            "Train loss 0.6022 accuracy 0.6846\n",
            "Val   loss 0.5713 accuracy 0.7101\n",
            "Epoch 5/5\n",
            "Train loss 0.5929 accuracy 0.7032\n",
            "Val   loss 0.5625 accuracy 0.7147\n",
            "\n",
            "--- Report for IE ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.84      0.77      0.81      1335\n",
            "     Class 1       0.41      0.53      0.46       400\n",
            "\n",
            "    accuracy                           0.71      1735\n",
            "   macro avg       0.63      0.65      0.63      1735\n",
            "weighted avg       0.74      0.71      0.73      1735\n",
            "\n",
            "\n",
            "========================================\n",
            " TRAINING RoBERTa (Frozen) FOR: NS\n",
            "========================================\n",
            "Class Weights: [0.58007355 3.62212944]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Layer Freezing Applied. Trainable Params: 14,767,874 / 124,647,170 (11.8%)\n",
            "Epoch 1/5\n",
            "Train loss 0.6972 accuracy 0.6275\n",
            "Val   loss 0.5986 accuracy 0.8542\n",
            "Epoch 2/5\n",
            "Train loss 0.6633 accuracy 0.6523\n",
            "Val   loss 0.5162 accuracy 0.7562\n",
            "Epoch 3/5\n",
            "Train loss 0.6286 accuracy 0.6821\n",
            "Val   loss 0.4744 accuracy 0.7890\n",
            "Epoch 4/5\n",
            "Train loss 0.6032 accuracy 0.7108\n",
            "Val   loss 0.5868 accuracy 0.6801\n",
            "Epoch 5/5\n",
            "Train loss 0.5880 accuracy 0.7114\n",
            "Val   loss 0.5486 accuracy 0.7101\n",
            "\n",
            "--- Report for NS ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.91      0.74      0.82      1496\n",
            "     Class 1       0.24      0.51      0.33       239\n",
            "\n",
            "    accuracy                           0.71      1735\n",
            "   macro avg       0.57      0.63      0.57      1735\n",
            "weighted avg       0.81      0.71      0.75      1735\n",
            "\n",
            "\n",
            "========================================\n",
            " TRAINING RoBERTa (Frozen) FOR: TF\n",
            "========================================\n",
            "Class Weights: [1.08948195 0.9241012 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Layer Freezing Applied. Trainable Params: 14,767,874 / 124,647,170 (11.8%)\n",
            "Epoch 1/5\n",
            "Train loss 0.6401 accuracy 0.6277\n",
            "Val   loss 0.5928 accuracy 0.6847\n",
            "Epoch 2/5\n",
            "Train loss 0.5764 accuracy 0.7033\n",
            "Val   loss 0.5628 accuracy 0.7055\n",
            "Epoch 3/5\n",
            "Train loss 0.5484 accuracy 0.7220\n",
            "Val   loss 0.5553 accuracy 0.7233\n",
            "Epoch 4/5\n",
            "Train loss 0.5335 accuracy 0.7369\n",
            "Val   loss 0.5539 accuracy 0.7159\n",
            "Epoch 5/5\n",
            "Train loss 0.5141 accuracy 0.7497\n",
            "Val   loss 0.5655 accuracy 0.7205\n",
            "\n",
            "--- Report for TF ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.74      0.61      0.67       796\n",
            "     Class 1       0.71      0.81      0.76       939\n",
            "\n",
            "    accuracy                           0.72      1735\n",
            "   macro avg       0.72      0.71      0.71      1735\n",
            "weighted avg       0.72      0.72      0.72      1735\n",
            "\n",
            "\n",
            "========================================\n",
            " TRAINING RoBERTa (Frozen) FOR: JP\n",
            "========================================\n",
            "Class Weights: [1.26319621 0.82756976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -> Layer Freezing Applied. Trainable Params: 14,767,874 / 124,647,170 (11.8%)\n",
            "Epoch 1/5\n",
            "Train loss 0.6931 accuracy 0.5294\n",
            "Val   loss 0.6815 accuracy 0.6115\n",
            "Epoch 2/5\n",
            "Train loss 0.6762 accuracy 0.5804\n",
            "Val   loss 0.6377 accuracy 0.6398\n",
            "Epoch 3/5\n",
            "Train loss 0.6541 accuracy 0.6012\n",
            "Val   loss 0.6389 accuracy 0.6305\n",
            "Epoch 4/5\n",
            "Train loss 0.6397 accuracy 0.6386\n",
            "Val   loss 0.6469 accuracy 0.6357\n",
            "Epoch 5/5\n",
            "Train loss 0.6377 accuracy 0.6272\n",
            "Val   loss 0.6451 accuracy 0.6323\n",
            "\n",
            "--- Report for JP ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.53      0.57      0.55       687\n",
            "     Class 1       0.70      0.67      0.69      1048\n",
            "\n",
            "    accuracy                           0.63      1735\n",
            "   macro avg       0.62      0.62      0.62      1735\n",
            "weighted avg       0.64      0.63      0.63      1735\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------"
      ],
      "metadata": {
        "id": "6aU6VmD7vXea"
      }
    }
  ]
}